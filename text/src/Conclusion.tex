\chapter{Conclusion}

In this concluding chapter we revisit the research question, summarize the
results of this thesis and evaluate the contributions in the context of the
research question. Furthermore, we highlight opportunities for future work and
speculate how the contributions of this thesis can benefit the programming
language research community at large in the future.

\section{Research Question}
Widespread formalization and mechanisation of programming language meta-theory
is hindered by the large development costs. For further adoption, reducing the
costs is crucial. This leads us back to the research question laid out in the
beginning of this thesis:

\begin{center}
  \begin{minipage}{0.8\columnwidth}\bf
    How can we reduce the cost of mechanising the formal meta-theory of
    programming languages?
  \end{minipage}
\end{center}

This thesis promotes reuse as a way to reduce the mechanisation effort of
programming language meta-theory and investigates the two principled approaches
\emph{modularity} and \emph{genericity} to reuse in general, and the application
of these approaches to programming language theory in particular.

\section{Summary}

This section gives and overview of the thesis and its most important results. We
discuss the two pats of the thesis in turn.

\subsection{Modularity}
Part I of this thesis pursues the modularity approach and universal method for
modularization of functional programs on inductive datatypes and modularization
of induction proofs for properties of these programs. Specifically this thesis
contributed new results for modular representation of modular datatypes and on
reducing feature interaction for effectful semantics. Both contributions have
been evaluated using case studies. We discuss both contributions and the case
studies below.

%% This universal method is showcased by modularizing type-safety proofs for lambda
%% calculi with several different features.

\subsubsection{Modular Representation}
Modularizing proofs is particularly challenging since the proof-assistant
settings imposes several restrictions for logical consistency. This challenge is
addressed in work prior to this thesis in the \emph{Meta-Theory \`a la Carte
  (MTC)} \cite{mtc} framework which uses Church encodings to represent inductive
datatypes and families. This thesis develops and alternative approach using
datatype-generic representations that improves over MTC in adequacy, convenience
and compatibility with classical logic.

\subsubsection{Feature Interaction}
A concern in modularization is the interaction between two or more features.
Each new feature that is integrated potentially interacts with all previous
ones. As a result, extending existing developments exhibits a quadratic increase
in effort. This is not a problem that is specific to a modular approach, but
applies to software development in general. However, it is an obstacle to
modularity if the interaction involves the complete set of features instead of
an isolated subset, e.g. two features.

Problematic feature interactions appear in the semantics of programming
languages with effectful features. To cut down this particular kind of
interaction, we developed a denotational semantics based on monad transformers
and corresponding algebraic laws. This semantics allowed us to to formulate and
prove a general type-soundness theorem in a modular way that enables the modular
reuse of language feature proofs and reduce the interaction between effectful
language features to the interaction between their effects.

\subsubsection{Case Studies}

We performed two case studies to evaluate our contributions. The first case
study is a port of the MTC case study to the container based modular
representation. It consists of continuity and type-safety proofs for seven
language features and six language compositions. In contrast to MTC, the
preservation of the universal properties or any other property is not necessary
for induction which makes the approach conceptually simpler. This is reflected
in the case study: sigma types and its projections are removed from lemmas and
proofs, and language features and their (proof) algebras are on average 240 LoC
(25\%) smaller. The final language compositions are slightly larger by about 6
LoC (7\%), but are short in comparison to the feature specific code.

The second case study demonstrates the 3MT framework. It consists of five
reusable language features: pure boolean and arithmetic features and effectful
features for references, errors and lambda abstractions. The study builds twenty
eight different combinations of the features. Each language feature has its own
reusable feature theorem and each of the six effect combinations its own effect
theorem. Effect theorems are only reusable for languages with the specific set
of effects. Like in the first case study, the feature and effect theorems
outweigh the final language compositions.



\subsection{Genericity}
Part II examines the genericity approach with a specific use case in mind:
variable binding boilerplate in mechanisations. A key ingredient of
reduction-based semantics of programming languages is substitiution.
Meta-theoretic proofs using these semantics usually involves a large number of
burdensome boilerplate proofs about substitutions which distracts a human
semanticist from essential theorems when mechanising her proofs. This thesis
develops a generic solution to the boilerplate lemmas based on a novel
specification language \Knot for programming language and a code generator
\Needle that produces boilerplate code. We summarize and discuss the
contributions.

\subsubsection{Specification Language}
\Knot allows the specification of abstract syntax, with explicit specifications
of binding, and of semantical relations between syntax terms. Relations are
defined using arbitrary expressions build from a language's abstract syntax.
\Knot employs a novel type system that checks that all expressions, including
substitutions that may appear in them, are always well-scoped.


\paragraph{Free Monadic Structure}
A central contribution of this thesis is the identification of a large class of
syntaxes for which boilerplate is completely generically derivable: syntactic
sorts that have a free monad-like structure. For relations this translates to
context parametricity of regular (non-variable) rules.

\paragraph{Principled Elaboration}
\Needle produces specialized definitions for a given \Knot specification in the
\Coq proof assistant and produces code for boilerplate functions and boilerplate
lemmas. We employ a principled approach to elaboration of boilerplate code that
gives us confidence in the correctness of our code generator: we developed and
formally verified elaborations in the \Coq proof assistant using our
datatype-generic implementation \Loom of \Knot.

\paragraph{Case Study}
Our case study compares our generic approach against fully manual mechanisations
of type safety proofs of 10 lambda calculi. It shows substantial savings in the
mechanisation for all 10 calculi with the largest savings being about 74\%
reduction in code size for System~F. This case study indicates that replacing
manual variable binding boilerplate by reusable generic solutions is indeed an
effective means of reducing the mechanisation effort.


\section{Future Work}

This section presents possible directions for further research. We group the
ideas around the two parts of the thesis and also discuss the combination of the
two parts.

\subsection{Modularity}

We achieved modular reuse of structurally recursive functions and structural
induction proofs. This is a crucial first step towards reducing mechanization
effort using the modularity methodology. However, the focus was on feasibility.
Further research is necessary to make the approach more convenient and
practical. Furthermore we targeted a specific class of languages (simply-typed
and effectful lambda calculi), specific semantics ((monadic) denotational
semantics) and specific meta-theoretic properties (type safety). Of course the
scope can be extended in either of these directions. We discuss the most
promising ideas for increasing the convenience and scope below.

\subsubsection{Convenience}

Our case study contains a substantial amount of bookkeeping of the relationship
of final and intermediate compositions of datatypes, relations, algebras and
proof algebras and similarly between the final and intermediate compositions of
effects for our monadic denotational semantics. Most of this is of course
boilerplate code that should be taken care of automatically.

Immediate and easy is to provide better specialized proof automation, for
instance for type class derivations and algebra dispatch proof steps.
Furthermore, a lot of the repetitive abstractions over super functors and
algebras can be dealt with available modularity features in proof assistants
like modular, module functors and canonical substructures.

Ultimately, the support for modular induction proofs can be added to proof
assistants to make this method practical on a larger scale. Concepts like
\emph{proof algebras} presented in this thesis can be turned into first-class
primitives of the proof-assistant's language. The generic universe
implementation of the first part can be used as a basis for an elaboration into
a core calculus.


\subsubsection{Scope}

As previously discussed, our case studies covered type safety proofs for
simply-typed lambda calculi with a (monadic) denotational semantics. We discuss
different possible extensions.

\paragraph{Languages}
An obvious extension would be to investigate the modularizability and feature
interaction of richer language, be it with more expressive type systems, e.g.
involving qualified types, polymorphism, dependent types or sub-typing, with
more complex language features like datatypes or modules, or with a different
principal programming paradigm like object-oriented or logical programming
language calculi.

\paragraph{Metatheoretic Properties}
Another extension of our work, is to modularize other meta-theoretic proofs than
type safety.

As laid out in the introduction of the thesis, an important direction is
verified compilation of programs. This can be combined with the modularity
methodology: write compilers that are modular in the source -- or even source
and target language -- and modularly verify semantically correct compilation.

Other interesting proofs are logical relation-based proofs of type safety,
strong normalization, parametricity or full-abstraction. Logical relations are
defined by structural recursion on the type structure of languages and should
therefore be easy to modularize. However, feature interactions in the proofs
still depend on the language features and it remains to be seen how easy it is
to modularize the interactions.

\paragraph{Semantics}
In our case studies we modularized type safety proofs and reduced non-modular
feature interaction between effectful features using a monadic denotational
semantics. Monads are, however, not the only approach to model side-effects and
it would be interesting to see how modularizable alternative approaches are.
Since our monadic typing rules with explicit continuations are reminiscent of
algebraic effects and handlers \cite{eff,handlers,hia}, this is one particularly
promising alternative to model effects. Furthermore, since algebraic effects
handlers have better composability properties than monads, they potentially
allow further reduction of feature interaction.


\subsection{Genericity}


\subsubsection{Scope}
There are multiple possibilities to grow the \Knot specification language to
handle more object languages. For instance, we can extend \Knot with support for
programming and let the user write functions. These functions could then also be
used in the definition of relations by extending the expression language
accordingly. Additionally, we can import more concepts like GADTs and derive
boilerplate for intrinsically well-typed syntax and include dependent types in
the expression language.

Furthermore, we can integrate more variable binding like concepts into \Knot,
for instance, first-class substitutions. This will also require us to improve
\Knot scope-checking type system.

The \Needle the code generator can be scaled to include the above extensions.
However, \Needle can also be scaled independently. Currently, the code is geared
towards type safety proofs, but other meta-theoretic proofs may require
different boilerplate that could be generated by \Needle. Furthermore, \Knot
itself is independent of a particular representation. Hence, we can imagine
generating boilerplate for different representations like a nominal,
locally-named or locally-nameless representation.


\subsubsection{Mathematical Foundations}

Last but not least, it would be interesting to explore the mathematical
foundations that underly \Knot. Categorical models exist for syntax with
variable binding, but \Knot's features exceed what can be described in these
models. For instance, the usual limitation is that only one namespace is
assumed.

A promising area is the modelling of well-scoped terms as a generalization of
relative monads. This is all the more interesting, since there is already
research available that models semantic relations as modules over relative monads.

%% \subsection{Combination}

%% For both methods, modularity and genericity, we used a datatype-generic
%% representation as part of the approach. This suggest that we may combine both.
%% In fact, this is an instance of the idea presented in Section
%% \ref{sec:pred:polynomialequality} where we complemented modular definitions with
%% generic definition of equality testing. Similarly, we can use \Loom to define
