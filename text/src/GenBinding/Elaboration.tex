\chapter{Elaboration}\label{ch:elaboration}
For the meta-theoretical formalization of programming language we need
properties of the semantical definitions. For instance, for type-theory proofs
via progress and preservation we generally need substitution properties of the
typing relation, and all the other boilerplate that is necessary to prove it.

There are multiple ways to the boilerplate lemmas for use in mechanizations. We
can develop a \emph{code generator} that produces code for a proof-assistant.
Alternatively, we can develop a \emph{datatype-generic} library inside a
proof-assistant. Both approaches have different trade-offs.

%% \item Programming language mechanizations typically rely on many boilerplate
%%   properties of the boilerplate operations that we introduced in the previous
%%   chapter.

\paragraph{Generic Library}

Alternatively, we can with (well-formed) \Knot specifications as the codes of a
generic universe. The main benefit of a library is, that its code, including all
proofs, can be checked separately independently of a concrete specification.
Hence, instantiating the library is guaranteed to yield valid theorems for all
specifications. Beyond providing strong correctness guarantees this approach has
other merits. A generic library provides evidence for the existence of
type-sound boilerplate definitions and proofs for for every language specified
with \Knot. Furthermore, it shows that \Knot embodies all necessary information
and constraints to derive the boilerplate.

Disadvantages of a generic library:
\begin{itemize}
\item Hiding of the internals. Do not expose the user of the library
  to its complexity. Ideally, the user does not have to be an expert
  in datatype-generic programming to use the library.

\item Constraints like well-formedness of binding specifications and expressions
  needs to be part of the universe code. Looking up meta-variables can
  potentially introduce partiality which needs to be dealt with.

\item Generically handling relations with arbitrary arities is challenging.

\item Induction over the generic generic representation is usually not the
  desired one. Do not use datatypes of the extension directly, but instead write
  isomorphism to user defined types. This also applies to well-scopedness
  predicates and relations.

\item Generic implementation shines through. For example, during
  simplification, the generic implementation might be exposed.

\item The statement of a generic lemma may not unify with all goals it applies
  to, before it has been properly specialized to a given specification. This
  impacts automatic proof search.

\item Related to that is the problem of proof discovery. Conceptually, both pose
  not a problem since in theory a proof assistant could specialize the types of
  lemmas when instantiating the generic code for a specification. However,
  currently none of the available proof assistants provide such functionality.
\end{itemize}


\paragraph{Code Generators}
A code generator can parse textual \Knot specifications and produces code for
use in a proof-assistant. In terms of usability, there are clear advantages.

The code generator can produce definition of datatypes and functions that are
close to what a human prover might write manually. Hence, the user can inspect
the code to understand the provided functionality in detail. Moreover, since the
code generator can specialize the statements of lemmas to the given
specification, it is much easier for the user to look at the provided proofs
and for proof automation to apply the provided proofs.

The main downside of a code generator is that the output still needs to be
checked. This raises the question if correct code is generated in all cases.
There are multiple ways to implement proofs; depending on the available methods
in the proof assistant. For instance, \Coq has a powerful scripting language to
automate writing proofs. However, very generic proof scripts tend to be brittle
and can possibly take a long time to execute. Furthermore, reasoning about the
correctness of the code generator not only involves the language of the proof
assistant, but also that of the scripting system.


\paragraph{Hybrid Approaches}

The main trade-off between the approaches is the confidence in a generic
implementation and the usability of a code generator. Ideally, we could combine
both approaches in order to get the advantages of both, and also mitigate their
disadvantages.

The obvious approach is to develop a generic library and address the usability
problems by developing a user-friendly code generator around it. The code
generator can instantiate the library with a user-defined specification and
specialize the generic definitions and lemmas as much as possible to hide the
complexity of the library. However, the technical challenges to develop a
generic library remain.

To ensure that all generic definitions are hidden, we need to generate
specialized definitions for every generic definition that appears in the user
facing interface. This ranges from obvious user-defined things like datatype
declarations for sorts, environments and relations to generically derived
inductive definitions like domains, well-scopedness relations and environment
lookups. This is crucial in case the user wants to perform induction over these
definitions. However, this also add duplication between the code-generator and
the generic implementation.
\footnote{In our case-study we used induction over well-scopedness of types to
  prove reflexivity of algorithmic subtyping of \fsub\xspace and induction over
  type variable lookups to prove that \fomega\xspace types that appear in a
  typing judgement are always well-kinded.}

Simplification of boilerplate functions like environment concatenation, shifting
and substitution may still reveal the generic implementation. In \Coq this
should be easily addressable with customized tactics.

An alternative approach, is to use a code-generator as the principle
implementation and try to transfer as much confidence in correctness as possible
from the generic approach. This is the path that we took and the remainder of
this chapter outlines our methodology to do so. We have built both, a
datatype-generic library called \Loom in \Coq, and acode generator named \Needle
that produces \Coq code and is implemented in Haskell.


\paragraph{Methodology}
\Loom defines de Bruijn interpretation of \Knot specification generically and
also implemented boilerplate functions and lemmas generically. Elaborating
datatype and function definitions in \Needle is comparably easy. The main
obstacle, that we need to solve, is how to turn a generic proof of \Loom into an
elaboration function in \Needle. One can aim to implement an elaboration that
follows the \emph{same strategy as the generic proof}. However, the question
remains if this is a faithful implementation of the same proof and if it indeed
is correct in all cases, especially when proof scripts are used.

We solve this problem by factoring the generic proofs of \Loom into elaboration
proofs. The first part are elaborations that produce for any given specification
specialized proofs and the second part are formal verifications of the
correctness of the elaborations.

Since the internals of the proofs assistant are not directly accessible for
elaboration or formal verification, and the language of the proof assistant is
too big, we choose a different target for the elaboration: namely intermediate
domain-specific witness languages. For each class of lemmas we develop a new
witness language specific for that class.

After developing the elaboration functions, their formal verification can be
divided into three different parts, which together represent alternatives to the
generic proofs:
\begin{enumerate}
%% \item A (syntax-directed) elaboration into a witness language that we designed
%%   for the given class of lemmas.
\item A formal semantics of the witness langauge.
\item The correctness of the elaboration, i.e. the produced witness indeed
  encodes a proof of the desired lemma w.r.t. the chosen semantics.
\item A soundness proof of the semantics of the witness language, e.g. the
  statement that a witness proves w.r.t. to the chosen semantics is valid (in
  our meta-language).
\end{enumerate}

Subsequently, we can implement the same elaborations in the code generator
\Needle. However, this does not establish formal correctness of the code
generator \Needle. The confidence in the correctness of \Needle still relies on
some factors.

\begin{enumerate}
\item \emph{The elaboration functions from \Knot specifications to the witness
  languages is correctly ported from \Coq to \Haskell.} The elaborations are
  recursive and pure functions\footnote{In reality, the elaboration functions
    are exclusively folds.} over algebraic dataypes which both \Coq and \Haskell
  support. Therefore, we port the elaboration code by literal translation from
  \Coq to \Haskell. In fact, most \Haskell elaboration functions were derived by
  a copy \& paste of the \Coq code.

\item \emph{The elaboration of the witness language into the language of the
  target proof-assistant is correctly implemented.} For our witness languages,
  this translation is straightforward, it is compositional and each witness
  primitive is either implemented by an application of a lemma or a rule.

\item The formal elaborations target only very specific parts of the boilerplate
  lemmas. For instance, our formal proof elaborations exclusively deal with
  inductive steps of induction proofs. Setting up the induction itself is done
  in unchecked \emph{glue code}. However, in comparison this glue code is less
  fragile.
\end{enumerate}



\paragraph{Overview}

There are too many lemmas to give an exhaustive account of all the
necessary elaborations. Instead we concentrate on representative and key lemmas.

The first sections in this chapter show how the witness language methodology is
applied to several classes of boilerplate lemmas. Section
\ref{ssec:elab:interaction:overview} discusses common interaction lemmas between
syntactic operations. Section \ref{sec:elab:wellscopedness} deals with the
well-scopedness lemmas that we discussed in Section
\ref{sec:gen:overview:formalization:semantics} and
\ref{sec:gen:formalization:metatheory}, i.e. lemmas that prove that indices of
relations are always well-scoped. Section \ref{sec:elab:shifting} covers
shifting and substitution lemmas for relations. Each of the Sections looks at
the class of lemmas that is being discussed, the domain-specific witness
language for that class, an elaboration for a representative lemma, and the
verification of the correctness of the elaboration.

Finally, Section \ref{sec:elab:impl:generic} discusses the generic library
implementation \Loom of \Knot in \Coq, and Section \ref{sec:elab:impl:codegen}
covers the implementation of our code generator \Needle.


%% \stevennote{MOVE}{To further reduce the hand-written boilerplate, we have set up
%%   the \Knot specification language in such a way that it provides all the
%%   necessary information to generically state and prove a wide range of these
%%   properties.}

%% \footnote{In fact, we provide more such lemmas than any other framework based on
%%   first-order representations -- see Section~\ref{sec:related}.}

%% \stevennote{MOVE}{Below we discuss different kinds of ubiquitous lemmas that
%%   \Needle covers; Appendix~\ref{app:lemmas} provides more detail.}

%% \stevennote{MOVE}{It is quite challenging to tackle these boilerplate lemmas
%%   generically because their exact statements, and in particular which premises
%%   are needed, depend highly on the dependencies between sorts and of the
%%   associated data in environments.}

\input{src/GenBinding/Elaboration/Interaction} \clearpage
\input{src/GenBinding/Elaboration/WellScopedness}
\input{src/GenBinding/Elaboration/RelShifting}
\input{src/GenBinding/Elaboration/RelSubstitution}
\input{src/GenBinding/Elaboration/Implementation}
\input{src/GenBinding/Elaboration/Discussion}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
