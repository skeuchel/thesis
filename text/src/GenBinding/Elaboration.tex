\chapter{Elaboration}\label{ch:elaboration}
For the meta-theoretical formalization of programming language we need
properties of the semantical definitions. For instance, for proofs of
type-safety we generally need substitution properties of the typing relation,
among other boilerplate properties.

There are multiple ways to prove the boilerplate lemmas for use in
mechanizations. We can develop a \emph{code generator} that produces code for a
proof-assistant.  Alternatively, we can develop a \emph{datatype-generic}
library inside a proof-assistant. Both approaches have different trade-offs.

%% \item Programming language mechanizations typically rely on many boilerplate
%%   properties of the boilerplate operations that we introduced in the previous
%%   chapter.

\paragraph{Code Generators}
A code generator can parse textual \Knot specifications and generate
definitions and proofs for use in a proof-assistant. 

In terms of usability, this approach has are clear advantages.
The generated definitions of datatypes and functions can be
close to what a human prover would write manually. Hence, the user can inspect
the code to better understand the provided functionality. Moreover, since the
code generator can specialize the statements of lemmas to the given
specification, it is much easier for the user to look at the provided proofs
and for proof automation to apply the provided proofs.

The main downside of a code generator is that we do not know whether the
correct code is generated in all cases. As a consequence, the proof assistant
still has to check the generated code. 

\Coq enables a particular staged approach to code generation that leverages its
proof automation facilities. The idea is that the code generator does not directly
generate custom proof terms, but instead generates an invocation of a \Coq proof
script. When this proof script is run, it builds the actual custom proof term.
This reduces the development effort, because the generator does not have to
implement its own proof generation. Moreover, the approach is more flexible
because we can change the definitions of the scripts without changing the
generator.
However, generic proof scripts tend to be brittle and can take a
long time to execute. Furthermore, reasoning about the correctness of the code
generator not only involves the language of the proof assistant, but also that
of the scripting system.


\paragraph{Generic Library}
Alternatively, we can develop a datatype-generic library with (well-formed)
\Knot specifications as the codes of a generic universe. The main benefit of such a
library is, that its code, including all proofs, can be checked once and for all and 
independently of a concrete specification. Hence, instantiating the library is
guaranteed to yield valid theorems for all specifications. Moreover, it shows that
\Knot embodies all necessary information and constraints to derive the
boilerplate.

There are several downsides to this approach. For usability, it is important to
hide the internals of the library and its complexity, since the user should
not have expert knowledge about datatype-generic programming to
use the library. Yet, there are several ways in which the implementation and
its complexity may be exposed. Firstly, the simplification of
boilerplate functions may reveal the generic implementation. Secondly,
the provided induction principle over generically defined
datatypes is (usually) not the one the user would expect. 
% This applies not only
% to algebraic datatypes for sorts and environments, but also to inductive
% families for well-scopedness predicates, variable look-ups, and user-defined
% relations.
% \footnote{In our case-study we used induction over well-scopedness of types to
%   prove reflexivity of algorithmic subtyping of \fsub\xspace and induction over
%   type variable look-ups to prove that \fomega\xspace types that appear in a
%   typing judgement are always well-kinded.} 
The above problems are usually mitigated in datatype-generic programming by
working with user-defined types instead, and by writing isomorphisms
to the generic representation. This means however, that some definitions like
well-scopedness predicates, which are clearly boilerplate, have to be
user-defined as well which defeats the purpose of a generic solution to this
kind of boilerplate. Luckily, it is possible to make the interface of
datatype-generic libraries aware of isomorphisms so that the user is spared from
applying the isomorphisms manually.

Automation of proofs may also be impaired when using lemmas of a generic library 
in comparison to manually proved or generated lemmas. For instance, The
statement of a generic lemma may not unify with all goals it applies to, before
it has been properly specialized to a given specification. This impacts
automatic proof search, in particular for user-defined relations,
which can have arbitrary arities. Related to that is the problem of proof
discovery. If the user wants to look at the lemmas a library provides, he sees
only the generic lemmas instead of the ones specialized for his
specification. Conceptually, this should not pose a problem since in theory a
proof assistant could specialize the types of lemmas when instantiating the
generic code for a specification. However, currently none of the available proof
assistants provide such functionality.

Finally, developing a datatype-generic library for \Knot is technically highly challenging.
Constraints like well-formedness of binding specifications and expressions needs
to be part of the universe code. Looking up meta-variables can potentially
introduce partiality which needs to be dealt with. Furthermore, there is little
research on writing datatype-generic libraries at this scale: not only is
datatype-generic programming more established than datatype-generic proving, but
also, research on generic proofs usually deals with universes of algebraic
datatypes and does not extend to user-defined relations which are part of \Knot
specifications.

\paragraph{Hybrid Approaches}

The main trade-off of the above two approaches is between the confidence in the
correctness and the usability. Ideally, we could combine both approaches in
order to get the best of both worlds, and to mitigate the respective disadvantages.

The obvious hybrid approach is to develop a generic library and address the
usability problems by developing a user-friendly code generator around it. The
code generator can instantiate the library with a user-defined specification
and specialize the generic definitions and lemmas as much as possible. However,
the technical challenges to develop a generic library remain.
To ensure that all generic definitions are hidden, we need to generate
specialized definitions for every generic definition that appears in the user
facing interface. This ranges from obvious user-defined datatype
declarations for sorts, environments and relations to generically derived
inductive definitions like domains, well-scopedness relations and environment
look-ups. However, the specialization adds duplication between the
code generator and the generic implementation.

% Simplification of boilerplate functions like environment concatenation, shifting
% and substitution may still reveal the generic implementation. In \Coq this
% should be easily addressable with customized tactics.

An alternative hybrid approach, is to use a code-generator as the principle
implementation and to transfer as much confidence in correctness as possible
from the generic approach. This is the path that we take and the remainder of
this chapter outlines our methodology to do so. To bolster our
confidence in code generation we have first built \Loom, a
datatype-generic library in \Coq, and then transferred its proof term
generation to \Needle, a cod generator in Haskell that produces \Coq code.

\paragraph{Methodology}
\Loom defines de Bruijn interpretation of \Knot specification generically and
also implemented boilerplate functions and lemmas generically. Elaborating
datatype and function definitions in \Needle is comparably easy. The main
obstacle, that we need to solve, is how to turn a generic proof of \Loom into an
elaboration function in \Needle. One can aim to implement an elaboration that
follows the \emph{same strategy as the generic proof}. However, the question
remains if this is a faithful implementation of the same proof and if it indeed
is correct in all cases, especially when proof scripts are used.

We solve this problem by factoring the generic proofs of \Loom into elaboration
proofs. The first part are elaborations that produce for any given specification
specialized proofs and the second part are formal verifications of the
correctness of the elaborations.

Since the internals of the proofs assistant are not directly accessible for
elaboration or formal verification, and the language of the proof assistant is
too big, we choose a different target for the elaboration: namely intermediate
domain-specific witness languages. For each class of lemmas we develop a new
witness language specific for that class.

After developing the elaboration functions, their formal verification can be
divided into three different parts, which together represent alternatives to the
generic proofs:
\begin{enumerate}
%% \item A (syntax-directed) elaboration into a witness language that we designed
%%   for the given class of lemmas.
\item A formal semantics of the witness langauge.
\item The correctness of the elaboration, i.e. the produced witness indeed
  encodes a proof of the desired lemma w.r.t. the chosen semantics.
\item A soundness proof of the semantics of the witness language, e.g. the
  statement that a witness proves w.r.t. to the chosen semantics is valid (in
  our meta-language).
\end{enumerate}

Subsequently, we can implement the same elaborations in the code generator
\Needle. However, this does not establish formal correctness of the code
generator \Needle. The confidence in the correctness of \Needle still relies on
some factors.

\begin{enumerate}
\item \emph{The elaboration functions from \Knot specifications to the witness
  languages is correctly ported from \Coq to \Haskell.} The elaborations are
  recursive and pure functions\footnote{In reality, the elaboration functions
    are exclusively folds.} over algebraic dataypes which both \Coq and \Haskell
  support. Therefore, we port the elaboration code by literal translation from
  \Coq to \Haskell. In fact, most \Haskell elaboration functions were derived by
  a copy \& paste of the \Coq code.

\item \emph{The elaboration of the witness language into the language of the
  target proof-assistant is correctly implemented.} For our witness languages,
  this translation is straightforward, it is compositional and each witness
  primitive is either implemented by an application of a lemma or a rule.

\item The formal elaborations target only very specific parts of the boilerplate
  lemmas. For instance, our formal proof elaborations exclusively deal with
  inductive steps of induction proofs. Setting up the induction itself is done
  in unchecked \emph{glue code}. However, in comparison this glue code is less
  fragile.
\end{enumerate}



\paragraph{Overview}

There are too many lemmas to give an exhaustive account of all the
necessary elaborations. Instead we concentrate on representative and key lemmas.

The first sections in this chapter show how the witness language methodology is
applied to several classes of boilerplate lemmas. Section
\ref{ssec:elab:interaction:overview} discusses common interaction lemmas between
syntactic operations. Section \ref{sec:elab:wellscopedness} deals with the
well-scopedness lemmas that we discussed in Section
\ref{sec:gen:overview:formalization:semantics} and
\ref{sec:gen:formalization:metatheory}, i.e. lemmas that prove that indices of
relations are always well-scoped. Section \ref{sec:elab:shifting} covers
shifting and substitution lemmas for relations. Each of the Sections looks at
the class of lemmas that is being discussed, the domain-specific witness
language for that class, an elaboration for a representative lemma, and the
verification of the correctness of the elaboration.

Finally, Section \ref{sec:elab:impl:generic} discusses the generic library
implementation \Loom of \Knot in \Coq, and Section \ref{sec:elab:impl:codegen}
covers the implementation of our code generator \Needle.


%% \stevennote{MOVE}{To further reduce the hand-written boilerplate, we have set up
%%   the \Knot specification language in such a way that it provides all the
%%   necessary information to generically state and prove a wide range of these
%%   properties.}

%% \footnote{In fact, we provide more such lemmas than any other framework based on
%%   first-order representations -- see Section~\ref{sec:related}.}

%% \stevennote{MOVE}{Below we discuss different kinds of ubiquitous lemmas that
%%   \Needle covers; Appendix~\ref{app:lemmas} provides more detail.}

%% \stevennote{MOVE}{It is quite challenging to tackle these boilerplate lemmas
%%   generically because their exact statements, and in particular which premises
%%   are needed, depend highly on the dependencies between sorts and of the
%%   associated data in environments.}

\input{src/GenBinding/Elaboration/Interaction} \clearpage
\input{src/GenBinding/Elaboration/WellScopedness}
\input{src/GenBinding/Elaboration/RelShifting}
\input{src/GenBinding/Elaboration/RelSubstitution}
\input{src/GenBinding/Elaboration/Implementation}
\input{src/GenBinding/Elaboration/Discussion}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
