{ % STLCBOOL SCOPE

\newcommand{\stlcbool}{\ensuremath{\lambda_{\mathbb{B}}}\xspace}
\newcommand{\bool}{\text{bool}}
\newcommand{\true}{\textbf{true}\xspace}
\newcommand{\false}{\textbf{false}\xspace}
\newcommand{\ite}[3]{\textbf{if}~{#1}~\textbf{then}~{#2}~\textbf{else}~{#3}}
\newcommand{\typing}[3]{{#1} \vdash {#2} : {#3}}
\newcommand{\eval}[2]{{#1} \longrightarrow {#2}}
\newcommand{\evals}[2]{{#1} \longrightarrow^* {#2}}

\chapter{Introduction}

The concept of \emph{programming} can be defined as explaining to a computer how
to perform a particular task. The language of communication is known as a
\emph{programming language} and the explanation expressed in a programming
language is called a \emph{program}; collectively programs are known as
\emph{software}. Unfortunately, programming is notoriously error-prone and
software often unreliable. Given the ubiquity of computers and the pervasive use
of software in present-day society, this unreliability is a very costly and
sometimes critical issue.

A prominent way to address this situation is to improve the programming
process. In particular, it is important that the programmer can verify whether
the program she has written implements the intended tasks. This is usually
achieved by reasoning with the \emph{expected behaviour} of a program, which of
course requires a solid understanding of the programming language in which the
program is expressed.

\paragraph{Programming Language Specifications}
In practice most programming languages do not start out as well-defined entities
with an explicit meaning. Instead they are usually indirectly defined in terms
of a software artifact like a compiler or interpreter, written in an existing
language, that processes the programs of the new language according to some
implicit intended meaning.  Only after widespread use and when the implicit
definition becomes untenable, a language may go through a standardization
process where multiple stakeholders develop a common specification of the
language.
% , e.g. PHP was standardized in 2014~\cite{phpspec}.

% Another approach is develop specifications together with a reference
% implementation. These two approaches are not mutually exclusive: languages with
% an existing specification are extended in tandem with implementations.

Such a language specification has many advantages. For instance, it allows
different parties to develop software tools that process programs consistently,
and programmers to switch between tool vendors without a hitch.  Also, it allows
programmers to resolve ambiguities when reasoning about programs independent of
a particular implementation.


\paragraph{Programming Language Theory}
The specification of programming languages is subject to scientific study in the
field of \emph{programming language theory}. This field deals with all aspects
of language specifications: the design and implementation of their syntax and
semantics, and that of auxiliary systems such as type systems. In addition the
field also studies the \emph{meta-theoretical properties} (or meta-theory for
short) of languages. These meta-theoretical properties capture expectations and
coherence aspects of newly developed languages or language features, such as
useful safety guarantees that hold for all programs expressed in the language.

A prominent meta-theoretical property is \emph{type safety}, i.e., the absence
of dynamic type errors during execution. Because meta-theoretical properties
like type safety do not automatically hold for all programming languages, we
need to verify whether they actually hold for given languages. If they do not,
this often points to a flaw in the language's design or a misunderstanding of
the language's workings.

\paragraph{Mechanised Meta-Theory of Calculi}
Verifying meta-theoretical properties of programming languages is not a trivial
activity. Due to many subtle details and edge cases, proofs can easily become
unwieldy and getting the proof structure right requires a lot of effort. In
order to make the process more manageable, the field of programming language
theory has adopted several methodological measures.

Firstly, because full programming languages are too large to handle,
meta-theoretical analysis usually restricts itself to a subset of the language,
known as a \emph{calculus}, that contains the main features of interest for the
property at hand. Because calculi are much smaller so are the proofs of their
meta-theory. The downside is that results for a calculus do not always carry
over to the full language. Problems in the calculus often reveal problems in the
full language, but the absence of problems in the calculus does not guarantee
the same for the full language.  For example, the Java programming language was
long believed to be type-safe\footnote{Except for some deliberate defects like
  the co-variance of arrays.}  but the addition of generics made Java's type
system unsound~\cite{amin2016java}. This fact was not apparent for many years,
and contradicts the type safety results for several generic Java
calculi~\cite{igarashi2001featherweight,Cameron2008}.
% Another problem concerns the type-checking of Java
% programs. Generics made type-checking an undecidable property of
% programs~\cite{grigore2017}. Decidability of type-checking does not have an
% impact on properties that we get for type-checked programs, but for example it
% opens up attack vectors on (web) services that deal with Java code because the
% type checker may run forever.

Secondly, because pen-and-paper proofs are very error-prone and human reviewers
are not perfect at vetting them, meta-theoretical proofs are often written in
formal languages that can be automatically checked by software tools known as
\emph{proof assistants}. This process, known as ``mechanisation'', greatly
increases the confidence in the validity of meta-theory proofs. Unfortunately,
mechanisation does not address the large effort of proving properties, but
rather aggravates it as every little detail has to be spelled out.

\paragraph{Research Question}
Despite the mitigating efforts of the current state of the art, neither the
formal specification of programming languages nor their rigorously mechanised
meta-theoretical analysis are a widespread practice. The development costs are
still too steep to make this possible for realistic programming languages. This
leads us to the research question of this thesis:
\begin{center}
  \begin{minipage}{0.8\columnwidth}\bf
    How can we reduce the cost of mechanising the formal meta-theory of
    programming languages?
  \end{minipage}
\end{center}

The main approach put forward in this thesis is \emph{reuse}. Reuse is a common
approach in software engineering to reduce development cost and increase both
quality and reliability. The idea is to identify functionality or patterns that
are in common between different software systems, and to implement them only
once in a manner that can be shared by the different software systems and reused
in the development of new systems. We apply the same idea to programming
language meta-theory, identifying repeated functionality and patterns, and
implementing them only once in a way that can be used across proofs for
different languages.

% \begin{itemize}
% \item Current methodology.
% \item Why is the current methodology costly?
% \item Give an idea of the relative effort.
% \item What is the approach answer of this thesis?
% 
%   Increase reusability of code. Two strands: Modularity / Genericity.
% \end{itemize}

The remainder of this chapter provides a more detailed introduction to the
established methodology for mechanising the meta-theory of programming
languages\footnote{We refer the interested reader to introductory textbooks on
  the matter (e.g., \cite{tapl}) for more detail.} and points out opportunities
for reuse.

%% %-------------------------------------------------------------------------------
%% \section{Mechanisation}
%% Mechanising formal meta-theory in proof-assistants is crucial, both for the
%% increased confidence in complex designs and as a basis for technologies such as
%% proof-carrying code.  Formal reasoning in proof-assistants, also known as
%% mechanisation, has high development costs.
%%

%% \begin{itemize}
%% \item
%%   Derive properties for all programs written in a language, like memory-safety,
%%   type safety, resources-safety, termination, absence of deadlocks, race
%%   conditions and starvation.
%%
%% \item
%%   Prove correctness (preservation of semantics) of program analyses or compiler
%%   transformations.
%%
%%   by looking formally on the semantics, type systems and implementations like
%%   compilers or interpreters.
%% \end{itemize}

%-------------------------------------------------------------------------------
\section{Programming Language Specifications}\label{sec:intro:specification}

%% Standardization and functional or program specifications are an essential
%% methodology in systems engineering and software development. It serves as the
%% baseline for correctness of implementations. Furthermore, multiple
%% implementations that are correct with respect to a common specification are
%% inter-operable or compatible to a certain extent. The precise meaning depends
%% on the context and the requirements.
%%
%% For programming languages for example, we want to guarantee that the same
%% program is executable by different implementations and performs the same
%% tasks.  This does for example also ensures that program analyses and program
%% transformations, e.g. for optimization, can be developed independent of a
%% particular implementation and adopted by multiple implementations.
%%
%% In this thesis, we are interested in programming languages themselves and not
%% so much interested in their implementations. Hence, a specification of a
%% application binary interface, for example, to achieve binary compatibility of
%% modules compiled by different compilers is beyond the scope of this thesis.

Specifications of programming languages differ in detail and precision.
Industrial specifications of major programming languages are usually written in
natural language and cover every aspect of the language in detail. Despite the
fact that natural language leaves opportunities for ambiguity, elaborate
industrial specifications provide a good reference point for language users and
implementors. However, for meta-theoretical analyses more rigorous
specifications are necessary. For this purpose we use \emph{formal
  specifications} and a mathematical language to describe programming
languages. This provides the necessary precision and avoids the ambiguities of
natural languages.

This section explains necessary fundamental concepts for the formal
specification of programming languages by example of a small language \stlcbool:
a simply-typed lambda calculus with booleans. We specify the \emph{abstract
  syntax}, \emph{static type system} and \emph{evaluation} of \stlcbool using
inductive definitions. Along the way, we define the terminology and notational
conventions and make their meaning precise.

%-------------------------------------------------------------------------------
\subsection{Syntax}\label{ssec:intro:syntax}

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
    \begin{tabular}{lcll}
      $x,y$           & ::=    &                            & term variable    \\
      $\tau,\sigma$   & ::=    &                            & type             \\
                      & $\mid$ & $\tau \to \tau$            & function type    \\
                      & $\mid$ & $\bool$                    & boolean type     \\
      $e$             & ::=    &                            & term             \\
                      & $\mid$ & $\true$                    & true constant    \\
                      & $\mid$ & $\false$                   & false constant   \\
                      & $\mid$ & $\ite{e}{e}{e}$            & conditional      \\
                      & $\mid$ & $x$                        & term variable    \\
                      & $\mid$ & $\lambda x:\tau.e$         & term abstraction \\
                      & $\mid$ & $e~e$                      & term application \\
      $v$             & ::=    &                            & value            \\
                      & $\mid$ & $\true$                    & true constant    \\
                      & $\mid$ & $\false$                   & false constant   \\
                      & $\mid$ & $\lambda x:\tau.e$         & term abstraction \\
      $\Gamma$        & ::=    &                            & type context     \\
                      & $\mid$ & $\epsilon$                 & empty context    \\
                      & $\mid$ & $\Gamma, x:\tau$           & term binding     \\
    \end{tabular}
    \end{minipage}
  }
  \caption{\stlcbool syntax}
  \label{fig:intro:stlcboolsyntax}
\end{figure}

The syntax of \stlcbool is given in Figure \ref{fig:intro:stlcboolsyntax}. We
use a convention that is close to standard (extended Backus-Naur form)
grammars. Elided in the grammar are syntactic constructs like parentheses. Yet
we use parentheses freely to resolve ambiguities in terms even if the grammar
does not define them. Any implementation that deals with the concrete syntax of
a programming language has of course to be more rigorous.

The grammar in Figure \ref{fig:intro:stlcboolsyntax} defines several
\emph{syntactic sorts} for \stlcbool. The \emph{meta-variable} $e$ stands for
expressions of \stlcbool of which there are 6 different kinds. An expression can
either be a boolean constant $\true$ or $\false$, a conditional form
$\ite{e_c}{e_t}{e_e}$, an \emph{object-language variable} (represented by the
meta-variable $x$), the definition of a function as a \textlambda-abstraction
$(\lambda x:\tau.e)$ or the application of an expression $e_1$ to another
expression $e_2$. In the case of an abstraction $(\lambda x:\tau.e)$ the
\emph{scope} of the variable $x$ is the body of the abstraction $e$. We will
also say that $x$ is bound in $e$ and more generally that this construct and the
\stlcbool language itself use \emph{variable binding}.

Of course, we only want to apply expressions $e_1$ that represent functions:
either by being a \textlambda-abstraction or evaluating to one. Applying any
\emph{value} other than a \textlambda-abstraction is a \emph{type error}. We
make this more precise below and come back to it in Section
\ref{sec:intro:typesafety} on analysis.

The grammar also includes the meta-variable $\tau$ that describes the types of
\stlcbool. Each \textlambda-abstraction contains a \emph{type annotation} $\tau$
for the argument variable $x$. The denotation is that the function represented
by the \textlambda-abstraction expects a value of type $\tau$ when it is
applied. We discuss types and typing contexts $\Gamma$ in more detail in Section
\ref{ssec:intro:typing}, which deals with static typing.


%-------------------------------------------------------------------------------
\subsection{Semantics}\label{ssec:intro:semantics}

We have defined the syntax of \stlcbool expressions and can now turn towards
defining their meaning. There are multiple established ways to define semantics
of programming language. We can coarsely classify the approaches into three
different groups:

\begin{enumerate}
\item Operational Semantics

  Operational semantics defines the meaning of programs by specifying their
  execution in a state transition system. A \emph{state transition function} or
  a \emph{state transition relation} on the terms of the programming language
  defines the possible execution steps. The program is part of the state. For
  small languages the entire state might consist of only the program. After
  taking a step we are left with an updated state that includes a residual
  program.

  %% We can then define the meaning of programs to be the execution process on
  %% the abstract machine.

\item Denotational Semantics

  Denotational semantics defines the meaning of programs in terms of collection
  of \emph{mathematical semantic domains} that can include numbers, sets or
  functions. An \emph{interpretation function} maps program terms into these
  domains. This function is generally compositional in the syntax which is
  beneficial for modularity.

  Usually, the semantic domain has an established \emph{formal theory}. The
  theorems of the domain give rise to reasoning laws for programs. Furthermore,
  we can also derive properties of programming languages from properties of the
  collection of semantic domains.

\item Axiomatic Semantics

  Instead of deriving laws for programs from their execution behaviour or
  denotation we can also axiomatically assume these laws. This is known as an
  axiomatic semantics.

  This gives us immediately the means for reasoning about programs. Furthermore,
  we can derive a denotational semantics for the language by constructing a
  model that satisfies the chosen laws and derive properties for this model or
  even all models.

\end{enumerate}

These three approaches have different trade-offs. Denotational and axiomatic
semantics immediately give us powerful mathematical tools to reason about
programming languages and their programs, but for larger languages the required
technicality and complexity makes it extremely difficult to even define a
suitable semantics.

Operational semantics do not give us the same powerful mathematical reasoning
techniques and instead impose on us the laborious task to reason about programs
by observing their behaviour during execution. However, operational semantics
are simpler and easier to define than more abstract denotational or axiomatic
semantics. Moreover, they are much closer to actual implementations. Due to the
smaller gap, operational semantics make it easier to reason about the
correctness of implementations.

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
      \framebox{\mbox{$\eval{e}{e}$}}\\
      \[ \begin{array}{c}
           \inferrule* [right=\textsc{EIfTrue}]
             {
             }
             {\eval{\ite{\true}{e_t}{e_e}}{e_t}
             } \\\\
           \inferrule* [right=\textsc{EIfFalse}]
             {
             }
             {\eval{\ite{\false}{e_t}{e_e}}{e_e}
             } \\\\
           \inferrule* [right=\textsc{EIf}]
             {\eval{e_c}{e_c'}
             }
             {\eval{\ite{e_c}{e_t}{e_e}}{\ite{e_c'}{e_t}{e_e}}
           } \\\\
           \inferrule* [right=\textsc{EAppAbs}]
             {
             }
             {\eval{(\lambda x:\tau.e_1)~e_2}{[x \mapsto e_2]e_1}
             } \\\\
           \inferrule* [right=\textsc{EApp}]
             {\eval{e_1}{e_2'}
             }
             {\eval{e_1~e_2}{e_1'~e_2}
             } \\\\
         \end{array}
       \]
    \end{minipage}
  }
  \caption{\stlcbool reduction rules}
  \label{fig:intro:stlcbooleval}
\end{figure}

For our \stlcbool language we define semantics using a \emph{small-step
  operational semantics}. This is also the approach used in Part \ref{part:gen}
of this thesis. Part \ref{part:mod} uses denotational semantics.

Figure \ref{fig:intro:stlcbooleval} gives the complete definition of the
operational semantics by means of an evaluation relation. The box in the upper
left corner \framebox{\mbox{$\eval{e_1}{e_2}$}} defines the shape and notation
that we use for the relation. In this case it is a binary relation between two
terms, which denotes that $e_1$ evaluates to $e_2$ in one step.

The remainder of the figure defines the relation using Gentzen-style inference
rules~\cite{gentzen1935}. In general, rules take the form
\[
  \inferrule* [right=\textsc{Name}]
    {J_1 \\ J_2 \\ \ldots \\ J_n
    }
    {J
    }
\]
where \textsc{Name} is an optional name for the rule that allows us to refer to
it. The meta-variable $J$ stands for judgements, which in our case are usually
mathematical statements in propositional or first-order logic. The judgements
$J_1, \ldots, J_n$ above the horizontal line are the premises of the rule, and
the judgement $J$ below the line is the conclusion. Rule without any premises
are also called \emph{axioms}. The conclusion will always involve the relation
that is being defined.

The single-step evaluation is defined using five rules. The two axioms
\textsc{EIfTrue} and \textsc{EIfFalse} show how to reduce an
\textbf{if}-expression in case the condition is either a \true or a \false
value. The case of a condition that is not yet fully evaluated is handled by
rule \textsc{EIf}. If the condition $e_c$ reduces to $e_c'$ in one step then we
can conclude the one step reduction
\[ \eval{\ite{e_c}{e_t}{e_e}}{\ite{e_c'}{e_t}{e_e}} \]

The last two rules cover the evaluation of \textlambda-terms. The rule
\textsc{EAppAbs} handles the case where the left-hand side of an application is
a \textlambda-term $(\lambda x:\tau.e_1)$. The residual program is the the body
$e_1$ of the function after substituting $e_2$ for $x$ which we write as $[x
  \mapsto e_2]e_1$. The argument of the function does not have to be fully
evaluated, i.e., the rules encode a \stevennote{MENTION
  EARLIER}{\emph{call-by-name}} evaluation strategy. If the left-hand side is
not yet a \textlambda-term, we evaluate it first similarly to \textsc{EIf}.

Note that this definition does not cover all possible cases. In particular the
case of a \textlambda-term in the condition of an \textbf{if}-expression
\[ \ite{(\lambda x:\tau.e)}{e_t}{e_e} \]
\noindent and the cases of a boolean in the left-hand side of an application
\[ \true~e_1 \qquad \text{or} \qquad \false~e_2 \]
\noindent are not specified. Since no transition is defined and the execution is
stopped without any \emph{meaningful result}, we also say that the evaluation
got stuck. In an implementation of the programming language, this corresponds to
an error that can happen during the execution of a program. It's therefore also
called a \emph{(dynamic) type error}. Programmers want to detect potential
problems like that early in the development cycle and, if possible, at compile
time. This motivates the development of \emph{static type systems}.

%-------------------------------------------------------------------------------
\subsection{Typing}\label{ssec:intro:typing}

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
      \framebox{\mbox{$\typing{\Gamma}{e}{\tau}$}}\\
      \vspace{-5mm}
      \[ \begin{array}{c}
           \inferrule* [right=\textsc{TTrue}]
             {
             }
             {\typing{\Gamma}{\true}{\bool}
             } \quad
           \inferrule* [right=\textsc{TFalse}]
             {
             }
             {\typing{\Gamma}{\false}{\bool}
             } \\\\
           \inferrule* [right=\textsc{TIf}]
             {\typing{\Gamma}{e_c}{\bool} \\
              \typing{\Gamma}{e_t}{\tau} \\
              \typing{\Gamma}{e_e}{\tau}
             }
             {\typing{\Gamma}{\ite{e_c}{e_t}{e_e}}{\tau}
             } \\\\
           \inferrule* [right=\textsc{TVar}]
             {x : \tau \in \Gamma
             }
             {\typing{\Gamma}{x}{\tau}
             } \quad
           \inferrule* [right=\textsc{TAbs}]
             {\typing{\Gamma,y:\sigma}{e}{\tau}
             }
             {\typing{\Gamma}{(\lambda y:\sigma. e)}{(\sigma\to\tau)}
             } \\\\
           \inferrule*[right=\textsc{TApp}]
             {\typing{\Gamma}{e_1}{\sigma \to \tau} \\
              \typing{\Gamma}{e_2}{\sigma}
             }
             {\typing{\Gamma}{e_1~e_2}{\tau}
             }
         \end{array}
       \]
    \end{minipage}
  }
  \caption{\stlcbool typing rules}
  \label{fig:intro:stlcbooltyping}
\end{figure}

A \emph{type system} is an assignment of types to expressions. Usually, not all
expressions are typeable and un-typeable expressions are rejected. Also, in some
languages there are expressions that can be assigned multiple, potentially
incomparable types. Both the partiality and the ambiguity of types suggest a
\emph{relational} rather than a \emph{functional} assignment. Such a relation is
defined in Figure \ref{fig:intro:stlcbooltyping}. It is a ternary relation
\framebox{\mbox{$\typing{\Gamma}{e}{\tau}$}} between a typing context $\Gamma$,
an expression $e$ and a type $\tau$.

The typing relation is defined using six rules. The two rules \textsc{TTrue} and
\textsc{TFalse} respectively state that the boolean constants $\true$ and
$\false$ have a boolean type. The rule \textsc{TIf} handles the case of an
\textbf{if}-expression. The three sub-expression positions contain
meta-variables $e_c$, $e_t$ and $e_e$. The premises require that the condition
$e_c$ has type boolean and the \textbf{then} and \textbf{else} branches have the
same type $\tau$. The rule then concludes that the entire \textbf{if}-expression
also has type $\tau$.

\stevennote{Is including TVar here confusing?}{The three remaining rules deal
  with $\lambda$-abstractions.} The typing context $\Gamma$ is a list that
associates term variables with types. In the case of a variable, rule
\textsc{TVar} looks up the corresponding type in $\Gamma$. Rule \textsc{TAbs}
checks the body of a $\lambda$-abstraction in the context $(\Gamma,y:\sigma)$
which is the outside context $\Gamma$ extended with a pair for the
$\lambda$-bound variable $y$. The type of the $\lambda$-abstraction is the
function type $(\sigma \to \tau)$ between the argument type $\sigma$ and the
type of the body $\tau$. Finally, rule \textsc{TApp} requires that the left
expression of an application has a function type that is compatible with the
argument.


\paragraph{Example}
Consider the boolean negation function
\[
  \lambda (y:\bool). \ite{y}{\false}{\true}
\]

This function sends booleans to booleans and should therefore have the type
$\bool\to\bool$. Giving the above typing relations, we can repeatedly apply the
rules to get a typing derivation for this. At each step only one possible rule
applies. We can arrange the rule applications in a so called \emph{derivation
  tree} that illustrates the whole derivation. Using the abbreviation $\Gamma'
:= \Gamma, y:\bool$, we have the \stevennote{Try fitting the rule
  names}{following tree}:

\[
  \inferrule*
  { \inferrule*
    {
      \inferrule*{\;}{\typing{\Gamma'}{y}{\bool}} \\
      \inferrule*{\;}{\typing{\Gamma'}{\false}{\bool}} \\
      \inferrule*{\;}{\typing{\Gamma'}{\true}{\bool}} \\
    }
    {\typing{\Gamma'}{\ite{y}{\false}{\true}}{\bool}
    }
  }
  { \typing{\Gamma}{\lambda y:\bool. \ite{y}{\false}{\true}}{\bool \to \bool}
  }
\]


%-------------------------------------------------------------------------------
\section{Meta-Theoretical Analysis}\label{sec:intro:typesafety}

%% It is folklore in software development that bugs arise in both
%% implementations and in specifications. For instance, \cite{arts2015testing}
%% describes a war story on testing car software in which they found 227 bugs in
%% the \textsc{Autosar} specification (and in implementations).

As discussed before, we want to establish meta-theoretical properties that
provide us with evidence for coherence of different parts of our language and to
increase our confidence that a language is well-designed. This needs a rigorous
and formal analysis of the defined semantics of a programming language.

In this section we showcase meta-theoretical analyses with the help of our
example calculus \stlcbool. In order to prove these properties we first need to
understand how to reason about languages and their semantics. We therefore look
at standard reasoning principles for relations like our typing and evaluation
relations. We show how language properties can be expressed precisely and define
two properties for our language: \emph{determinacy of evaluation} and \emph{type
  safety}, which we define below. We give a complete proof of determinacy for
our language in order to demonstrate the reasoning principles. In the remainder
of the thesis the type safety property plays a prominent role. Below we present
an established way of proving type safety, namely through progress and
preservation lemmas, and sketch the proofs.


\paragraph{Inductive Reasoning}
For our meta-theoretical analyses, we first need to establish methods for
reasoning about the evaluation and typing of \stlcbool. Figures
\ref{fig:intro:stlcbooleval} and \ref{fig:intro:stlcbooltyping} respectively
define evaluation and typing for \stlcbool. More precisely, our intention in
both figures is to define the \emph{smallest relation} that includes the
presented rules. This gives rise to a \emph{structural induction principle} for
the relations. Put differently, we can induct over the shape of derivation trees
(or their size or height).

As an example consider the following determinacy theorem which states that at
each point there is at most one possible successor state.
\begin{thm}[Determinacy]
  If $\eval{e_1}{e_2} \wedge \eval{e_1}{e_3}$ then $e_2 = e_3$.
\end{thm}

\begin{proof}
  The proof proceeds by induction over the derivation of $\eval{e_1}{e_2}$. For
  each possible case in the derivation of $\eval{e_1}{e_2}$ we inspect the last
  rule that was used to derive $\eval{e_1}{e_3}$. If the same rule was used, we
  can derive the equation. All other combinations lead to a contradiction and
  hence the property follows.

  We go through one of the inductive steps in detail and omit the others for
  brevity. Consider the case where $\eval{e_1}{e_2}$ was derived using
  \textsc{EIf}. So there exist expressions $e_{11}, e'_{11}, e_{12}, e_{13}$
  such that
  \begin{gather*}
    e_1 = \ite{e_{11}}{e_{12}}{e_{13}}, \\
    e_2 = \ite{e'_{11}}{e_{12}}{e_{13}} \\
    \text{and}~\eval{e_{11}}{e'_{11}}.
  \end{gather*}

  Now look at the last rule that was used to derive $\eval{e_1}{e_3}$. There are
  two possibilities: rule \textsc{EIfTrue} or rule \textsc{EIf}.

  \begin{enumerate}
  \item If rule \textsc{EIfTrue} was used, then we learn that $e_{11} = \true$
    and therefore $\eval{\true}{e'_{11}}$. However this is impossible because no
    rule evaluates $\true$ further.

  \item If rule \textsc{EIf} was used, then there exists $e''_{11}$ such that
    \begin{gather*}
      \text{and}~\eval{e_{11}}{e''_{11}}.
    \end{gather*}

    Applying the inductive hypothesis of $\eval{e_{11}}{e'_{11}}$ to the
    derivation $\eval{e_{11}}{e''_{11}}$ gives us the equality
    $e'_{11} = e''_{11}$ from which we can derive $e_2 = e_3$.
  \end{enumerate}
\end{proof}

% More precisely we have to proof
% $\forall e_3. \eval{e_1}{e_3} \Rightarrow e_2 = e_3$.


\paragraph{Type Safety}

A programmer using a statically-typed language will expect certain safety
guarantees from the type system when her program is executed. Intuitively, an
expression of a given type will eventually evaluate to a value of that
type. Usually side-conditions are implicitly assumed like for example the
assumption that the computation will not diverge. In practice, there are
statically-typed languages that allow non type-safe programs to be written,
e.g. C and C++ are inherently unsafe. The overall convention is still that
programmers write type-safe code and assume that code written by others is
type-safe. In these situations type safety is a property of programs, but in
this section we want to make it a property of languages, or put differently, we
want that all programs of a language are type-safe.

Below we look at a formal definition of a type safety property that guarantees
that the expectations of the programmer are met. Our definition will be slightly
stronger than what a programmer might anticipate. The intuition is that we
disallow dynamic type errors during evaluation instead of focusing on the result
of evaluation.

We first make vague concepts like \emph{value}, or \emph{type error} precise.
Values are expressions that are canonical for the types of the language.

\begin{defn}[Value]
  Values are the subset of expressions that are defined by the following
  grammar:
  \begin{center}
    \begin{tabular}{lcll}
      $v$ & ::=    &                    & term             \\
          & $\mid$ & $\true$            & true constant    \\
          & $\mid$ & $\false$           & false constant   \\
          & $\mid$ & $\lambda x:\tau.e$ & term abstraction \\
    \end{tabular}
  \end{center}
\end{defn}

By inspecting all the rules of the evaluation relation, we can see that there
are no further transitions from a value. Values are thus fully
evaluated\footnote{Fully evaluated with respect to the given semantics. Redexes
  may still appear under $\lambda$-abstractions which are
  \emph{suspended}. There are also semantics that allow evaluation under
  $\lambda$-abstractions.} expressions. Such expressions are also called
\emph{normal forms}. More formally we have the following definition and lemmas.

\begin{defn}[Normal Form]
  An expression $e_1$ is a \emph{normal form} if no further execution step can be
  taken, i.e.,
  \[ \forall e_2. \neg (\eval{e_1}{e_2}) \]
\end{defn}

\begin{lem}[Values are Normal]
  If an expression $e$ is a value, then it's also a normal form.
  \begin{proof}
    By inspecting the evaluation rules for each value form.
  \end{proof}
\end{lem}

We can thus translate our intuitive understanding of type safety into a theorem:

\begin{thm}[Type Safety (First Attempt)]
  Let $e_1$ be an expression of type $\tau$,
  i.e. $\typing{\cdot}{e_1}{\tau}$. If $e_1$ evaluates in one or more steps to a
  value $e_2$, then $e_2$ also has type $\tau$:
  \[ \forall e_2. (\evals{e_1}{e_2} \wedge e_2~\text{is a value}) \Rightarrow
     \typing{\cdot}{e_2}{\tau}.
  \]
\end{thm}

This definition however is problematic to work with directly. First, we cannot
prove it directly by induction over $e_1$ or over the typing derivation
$\typing{\cdot}{e_1}{\tau}$, because in the case of an evaluation step with rule
\textsc{EAppAbs} we are given a reduced term that is not a sub-term and hence we
have no induction hypothesis available. Second, it does not express strong
coherence between typing and evaluation. Consider for instance the language that
we get, if we remove the evaluation rule \textsc{EIfTrue}. Then the expression
\[ \ite{\true}{e_t}{e_e} \]
\noindent is a normal form but not a value. However, it is different than for
example the expression
\[ \ite{(\lambda x:\tau.e)}{e_t}{e_e} \]
\noindent which is also a normal from but not a value. But intuitively,
evaluations that stopped should either be values or type errors.

With this understanding we can reformulate our type safety theorem. Type safe
languages rule out type errors; consequently, normal forms should already be
values.

\begin{thm}[Type Safety]\label{thm:stlcbool:typesafety}
  Let $\typing{\cdot}{e_1}{\tau}$. If $e_1$ evaluates to a normal form $e_2$
  then $e_2$ is a value of type $\tau$:
  \[ (\evals{e_1}{e_2} \wedge e_2~\text{is normal}) \Rightarrow
     (e_2~\text{is a value} \wedge \typing{\cdot}{e_1}{\tau}).
   \]
\end{thm}

Note that this definition does not require the evaluation to terminate. A
program that runs forever without getting stuck is also considered type-safe.

Proving this property directly is difficult as well. One established and popular
way is to reduce it to two simpler properties, namely \emph{progress} and
\emph{preservation}. Progress expresses that we can always take a step as long
as we do not reach a value.
\begin{lem}[Progress]
  Let $\typing{\cdot}{e_1}{\tau}$. Either $e_1$ is a value or we can take
  another step, i.e.
  \[ \exists e_2. \eval{e_1}{e_2}. \]
  \begin{proof}[Proof (Sketch)]
    By induction over the typing derivation $\typing{\cdot}{e_1}{\tau}$. If we
    can take an evaluation step in a sub-term we can use rule \textsc{EIf} rule
    \textsc{EApp}. Otherwise, we learn that we have a value in an evaluation
    position. By inspecting the possible values for a given type we can take a
    step with one of the rules \textsc{EIfTrue}, \textsc{EIfFalse} or
    \textsc{EAppAbs}.
  \end{proof}
\end{lem}

\begin{lem}[Preservation]
  If $\typing{\Gamma}{e_1}{\tau}$ and $\eval{e_1}{e_2}$ then
  $\typing{\Gamma}{e_2}{\tau}$.
  \begin{proof}[Proof (Sketch)]
    By induction over the typing derivation $\typing{\Gamma}{e_1}{\tau}$ and
    inspection of the last rule to derive $\eval{e_1}{e_2}$. The only
    interesting case is \textsc{EAppAbs} which follows from an additional lemma
    that states that typing is preserved by well-typed substitutions. In all
    other cases the property follows immediately from an induction hypothesis or
    by application of a single rule and the induction hypotheses.
  \end{proof}
\end{lem}

\begin{proof}[Proof of Theorem \ref{thm:stlcbool:typesafety}]
  By induction over the number of evaluation steps in $\evals{e_1}{e_2}$ and
  using the progress and preservation lemmas.
\end{proof}


%-------------------------------------------------------------------------------
\section{Mechanisation}

Meta-theoretical proofs are long and require the management of many details.
Consequently, these proofs are prone to error and it is easy for human verifiers
and reviewers to overlook mistakes. This is aggravated by the fact, that
properties tend to fail in subtle edge cases. For instance, the unsoundness
example of \cite{amin2016java} for Java involves the interplay between
constrained wildcards of generics and null references. Hence, there is a real
danger that an overlooked detail or a falsely assumed assumption leads to both
invalid proofs and invalid results.

\paragraph{Theorem provers}
In order to gain more confidence in the correctness of meta-theory proofs, the
field of programming language theory has started to adopt \emph{mechanization}
as a new methodology: writing mathematical theorems in a formal language and
verify correctness of their proofs mechanically by \emph{automated theorem
  provers}. Theorem provers may find proofs for theorems fully automatically or
require a human user to input the proof or proof hints. In the latter case, the
system is also called a \emph{proof assistant}. In both cases, every reasoning
step of the proof is verified to be valid in the system's underlying logic. This
has the benefit that gaps in the proof are ruled out and unproven assumptions
need to be explicitly documented. Therefore, the use of theorem provers greatly
increases the confidence in the validity of (meta-theory) proofs.

% Automation for first-order logics has seen the most development and
% fully-automatic theorem provers for various such logics exist
% \cite{robinson2001handbook}. Richer logics such as higher-order logics and type
% theories generally require the use of a proof assistant.

Among programming language researchers proof assistants are more popular,
because they provide richer logics. % for which proof search is harder to
automate. Systems like Abella~\cite{abella}, Agda~\cite{norellthesis},
Beluga~\cite{beluga}, Coq~\cite{coq}, Isabelle/HOL~\cite{isabelle} and
Twelf~\cite{twelf} have been used to check various meta-theoretic proofs. The
developments of this thesis have been done in the context of the Coq proof
assistant which offers a competitive degree of automation and is one of the most
widely used proof assistants for meta-theory.

\paragraph{Benefits}
To illustrate the benefits of mechanisations, consider the elaborate bug hunting
study of \citet{yang2011bugs} using their randomized test-case generator
\textsc{Csmith}.  During their study they found and reported over 325 bugs in 11
open source and commercial C compilers. One of the compilers is
\textsc{CompCert}~\cite{Leroy2009}, a C compiler implemented in the Coq proof
assistant~\cite{coq} and proven to be correct with respect to a formal
specification of the C programming language. \citet{yang2011bugs} write the
following about \textsc{CompCert}:

%% Improving the correctness of C compilers is a worthy goal: C code is part of
%% the trusted computing base for almost every modern computer system including
%% mission-critical financial servers and life-critical pacemaker firmware.

\blockquote{The striking thing about our \textsc{CompCert} results is that the
  middle-end bugs we found in all other compilers are absent. As of early 2011,
  the under-development version of \textsc{CompCert} is the only compiler we
  have tested for which \textsc{Csmith} cannot find wrong-code errors. This is
  not for lack of trying: we have devoted about six CPU-years to the task. The
  apparent unbreakability of \textsc{CompCert} supports a strong argument that
  developing compiler optimizations within a proof framework, where safety
  checks are explicit and machine-checked, has tangible benefits for compiler
  users.}

%% % SPEC
%% %%  43        0        3 BoilerplateFunctions.v
%% %%  35        0        2 SpecSemantics.v
%% %%  16        0        3 SpecSyntax.v
%% %%  94
%%
%% % THEORY
%% %%  10       12        2 BoilerplateSemantics.v
%% %%  20        3        3 BoilerplateSyntax.v
%% %%  25       20        3 MetaTheory.v
%% %%  55
%%
%% % TOTAL
%% %% 149       35       16 total

\paragraph{Costs}
Unfortunately, mechanisation does not address the large effort of proving
properties, but rather aggravates it as every little detail has to be spelled
out. The size depends largely on the the language and the properties that are
proved. Our example language \stlcbool can be specified in about 94
lines\footnote{Including definitions of capture-avoiding substitutions that we
  omitted for brevity.} of Coq code and type safety including all necessary
lemmas can be proven in 55 lines of code. However, this is not representative
since \stlcbool is a bare-bones calculus. It is not uncommon to see developments
with few hundreds to thousands of lines of language specifications and tens of
thousands of lines of meta-theory proofs \cite{Leroy2009, Zhao2010}.


%% The meta-theory of programming language semantics and type systems is highly
%% complex due to the management of many details. Formal proofs are long and prone
%% to subtle errors that can invalidate large amounts of work. In order to
%% guarantee the correctness of formal meta-theory, techniques for mechanical
%% formalization in proof-assistants have received much attention in recent years.
%%
%% %-------------------------------------------------------------------------------
%% \section{Mechanisation}
%% Mechanising formal meta-theory in proof-assistants is crucial, both for the
%% increased confidence in complex designs and as a basis for technologies such as
%% proof-carrying code.  Formal reasoning in proof-assistants, also known as
%% mechanisation, has high development costs.
%%
%% To lighten the burden of programming language mechanisation, many
%% approaches have been developed that tackle the substantial boilerplate which
%% arises from variable binders. Unfortunately, the existing approaches are limited
%% in scope.
%% %% STEVEN: This is still valid, but I hope it's not misleading the reader into
%% %% thinking we deal with typing relations directly.
%% As a consequence, the human mechaniser is still unnecessarily burdened with
%% binder boilerplate and discouraged from taking on richer languages.
%%
%% This paper presents \Knot, a new approach that substantially extends the support
%% for binder boilerplate. \Knot is a highly expressive language for natural and
%% concise specification of syntax with binders. Its meta-theory constructively
%% guarantees the coverage of a considerable amount of binder boilerplate for
%% well-formed specifications, including that for well-scoping of terms and context
%% lookups. \Knot also comes with a code generator, \Needle, that specializes the
%% generic boilerplate for convenient embedding in \Coq and provides a tactic
%% library for automatically discharging proof obligations that frequently come up
%% in proofs of weakening and substitution lemmas of type systems.
%%
%% Our evaluation shows, that Needle \& Knot significantly reduce the size of
%% language mechanisations (by 40\% in our case study). Moreover, as far as we
%% know, \Knot enables the most concise mechanisation of the \POPLmark Challenge
%% (1a + 2a) and is two-thirds the size of the next smallest. Finally, \Knot allows
%% us to mechanise for instance dependently-typed languages, which is notoriously
%% challenging because of dependent contexts and mutually-recursive sorts with
%% variables.
%%
%% %-------------------------------------------------------------------------------
%% \section{Binding}
%%
%% To lighten the burden of programming language mechanisation, many approaches
%% have been developed that tackle the substantial boilerplate which arises from
%% variable binders. Unfortunately, existing approaches for first-order
%% representations are limited to the boilerplate that concerns the syntax of
%% languages and do not tackle common boilerplate lemmas that arise for semantic
%% relations such as typing. Consequently, the human mechaniser is still burdened
%% with proving these substantial boilerplate lemmas manually.
%%
%% %% POPL 2014 Submission
%% %%
%% %%   A key concern in the mechanisation of programming language metatheory is
%% %%   the representation of terms with variable binding. The properties of
%% %%   operations manipulating terms are notoriously burdensome to prove and the
%% %%   amount of work required to scale formalizations to realistic programming
%% %%   languages with rich binding forms is deemed infeasible. This is a pity,
%% %%   because we lose the practical benefits of mechanising real programming
%% %%   languages.
%% %%
%% %%   We present a new solution to generically handle the boilerplate involved in
%% %%   mechanisations that scales to rich binding forms and advanced rules of
%% %%   scoping. We define a new specification language for abstract syntax with
%% %%   binding and implement a code generator that produces \Coq code for the
%% %%   representation of the abstract syntax, syntactic operations and proofs of
%% %%   their properties.
%% %%
%% %%   We illustrate how our approach removes the burden of variable binding
%% %%   boilerplate in the mechanisation of realistic programming languages on a
%% %%   list of example specifications and a solution of the PoplMark challenge
%% %%   based on the generated code.


%-------------------------------------------------------------------------------
\section{Reusability}

Despite the benefits, neither formal specification of programming languages nor
rigorously mechanised meta-theory proofs are widespread practice. One of the
main obstacles are the large development costs. This thesis aims to help spur
further adoption of formal mechanised meta-theory by promoting \emph{reuse} as a
method to lower the mechanisation effort.

Reuse is a common approach in software engineering to reduce development cost
and increase both quality and reliability. The idea is to identify functionality
or patterns that different software systems have in common and implement them
only once in a manner that can be shared by the different software systems and
reused in the development of new systems. We apply the same idea to programming
language meta-theory, identifying repeated functionality and patterns, and
implementing them only once in a way that can be used across proofs for
different languages.

Unfortunately, the current practice to achieve reuse is to copy an existing
formalization, \emph{change the existing definitions} manually, integrate new
features and to subsequently \emph{patch up the proofs} to cater for the
changes. This unprincipled approach to reuse leaves much to be desired. First,
editing and patching the existing definitions breaks \emph{abstraction}, a core
principle of computer science. Ideally, we would like to reuse existing code via
an interface that provides functionality (for programming) and properties (for
reasoning). Second, this approach does not encourage \emph{isolation} of new
features from existing ones, which hinders backporting improvements to the
existing formalization.

Our goal is to replace the current practice with principled ways to achieve
reusability. More specifically, this thesis is examining two different means of
reuse: 1. Through \emph{modularity} and 2. through \emph{genericity}.

\paragraph{Modularity}
Programming languages, just like regular software systems, can be described by
the functionality or features that they provide. The meta-theoretic development
of our example language \stlcbool consists of different functionality: the
syntax, semantics and its type safety proof. It easy to reuse the syntax and
semantics and prove other kinds of properties, e.g. termination, or, to reuse
the syntax and switch out the semantics and prove type safety for the new
semantics.

Furthermore, \stlcbool has two easily distinguishable features:
\textlambda-terms and boolean expressions. Many programming languages and their
calculi feature either or both of these language constructs among many others.
This commonality hints at another opportunity for reuse. However, achieving this
kind of reuse is challenging (cf. Section \ref{sec:mod:expressionproblem}).

Ideally, each feature could be developed in complete isolation and modularly
combined into a full language consisting of a specific set of features. The
reality, however, is more complicated. Features may have dependencies between
them or may interact with each other even though they seem to be orthogonal.

This thesis investigates the modularization of meta-theory proofs along feature
boundaries and specifically looks at the reduction of interaction between
side-effecting language features.


\paragraph{Genericity}
Names are found in almost every high-level programming language to refer to
classes, methods, types, functions, function parameters, etc. In case a name is
\emph{substitutable} we call it a \emph{variable}. A language may have multiple
kinds of variables, for example, term and type variables. The operational
semantics of languages with variables often implement reduction of language
constructs by means of \emph{substitution}.

Variable substitution is an operation that is not specific to a particular
language or language feature but is common to any language that uses variable
binding. The implementation of substitution follows a standard recipe that can
be applied for any specific language. This hints at a different way of achieving
reuse: by implementing substitution \emph{once} generically and specializing
this generic implementation to any given concrete language.

While other kinds of \emph{generic functionality} are commonly found in
programming language meta-theory, e.g. term equality or \stevennote{Cite TAPL
  and/or some generic rewriting paper}{(first-order) unification}, the generic
functionality considered in this thesis is substitution. Substitution is
interesting functionality for reuse because the need for substitutions arises in
nearly all meta-theory proofs for languages with variable binding. Furthermore,
many theorems need a large amount of lemmas about substitutions. Discharging
them automatically, e.g. through reusable generic implementations, can save a
lot of development effort.

\section{Overview}

This thesis is split into two parts, corresponding to the two means of achieving
reuse with the discussed focus.

\paragraph{Part I: Modularity}

Chapter \ref{ch:modbackground} presents the necessary background for this
part. While modular development of software is a well-studied topic in computer
science, modular composition of proofs is not as well-studied. Chapter
\ref{ch:modpreduniv} develops one approach to modularized algebraic dataypes and
modularized induction proofs for them. This chapter contains the material from

\begin{center}
  \begin{minipage}{0.85\columnwidth}
    Keuchel, S. and Schrijvers, T. (2013).
    \newblock Generic Datatypes \`a la Carte.
    \newblock In {\em Proceedings of the 9th ACM SIGPLAN workshop on Generic
      programming}, WGP ’13, pages 13-24. ACM.
  \end{minipage}
\end{center}

Side-effecting language features exhibit a lot of interaction, e.g. in
operational semantics, which hinders modularization. This is a problem that is
tackled in Chapter \ref{ch:modmoneff} by developing a monadic denotational
semantics for features with side-effects that can be modularized. This chapter
is based on the publication

\begin{center}
  \begin{minipage}{0.85\columnwidth}
    Delaware, B., Keuchel, S., Schrijvers, T., and Oliveira,
    B. C. d.~S. (2013).
    \newblock Modular Monadic Meta-Theory.
    \newblock In {\em Proceedings of the 18th ACM SIGPLAN international
      conference on Functional programming}, ICFP '13, pages 319-330. ACM.
  \end{minipage}
\end{center}


\paragraph{Part II: Genericity}
This part deals with the development of a generic substitution operation and
associated theorems. The solution put forward is a framework consisting of a
specification language \Knot for abstract syntax with variable binding and the
tool \Needle that compiles \Knot-specifications to Coq code which includes
substitution operators and proofs about substitutions. Chapters
\ref{ch:knotsyntax} and \ref{ch:knotsemantics} present the syntax and semantics
of \Knot respectively, while Chapter \ref{ch:elaboration} discusses the
elaboration and code generation underlying \Needle.

Part \ref{part:gen} is based on

\begin{center}
  \begin{minipage}{0.85\columnwidth}
    Keuchel, S., Weirich, S., and Schrijvers, T. (2016).
    \newblock Needle {\&} {K}not: {B}inder {B}oilerplate {T}ied {U}p.
    \newblock In {\em Programming Languages and Systems: 25th European Symposium
      on Programming}, ESOP '16, pages 419--445. Springer.
  \end{minipage}
\end{center}
\noindent which deals with substitutions at the term level and
\begin{center}
  \begin{minipage}{0.85\columnwidth}
    Keuchel, S.,  Schrijvers, T., and Weirich, S. (2016).
    \newblock Needle {\&} {K}not: {B}oilerplate {B}ound {T}ighter.
    \newblock Unpublished draft.
  \end{minipage}
\end{center}
\noindent which extends the framework further to include predicates on terms
expressed as relations.


\paragraph{Not included}

Related to the content of this thesis, but not included, are the articles

\begin{center}
  \begin{minipage}{0.85\columnwidth}
    Keuchel, S. and Schrijvers, T. (2012).
    \newblock Modular Monadic Reasoning, a (Co-)Routine.
    \newblock Presented at \emph{the 24th Symposium on Implementation and
      Application of Functional Languages}, IFL '12.
  \end{minipage}
\end{center}
\noindent which develops initial ideas to modular reasoning about side-effecting
components, and

\begin{center}
  \begin{minipage}{0.85\columnwidth}
    Keuchel, S. and Schrijvers, T. (2015).
    \newblock {I}n{B}ound: {S}imple yet powerful {S}pecification of {S}yntax
      with {B}inders.
    \newblock Unpublished draft.
  \end{minipage}
\end{center}

\noindent which aims to develop a richer specification language of syntax with
binding than \Knot, including binding constructs of realistic programming
languages that \Knot does not support.

Furthermore, the experiences gained from the work on \Needle \& \Knot are used
to tackle the substitution boilerplate of

\begin{center}
  \begin{minipage}{0.85\columnwidth}
    Devriese, D., Patrignani, M., Piessens, F., and Keuchel, S. (2017).
    \newblock {Modular, Fully-abstract Compilation by Approximate
      Back-translation}.
    \newblock {\em {Logical Methods in Computer Science}}, {Volume 13, Issue 4}.
  \end{minipage}
\end{center}


} % STLCBOOL SCOPE


% \item Firstly, because full programming languages are too large to handle,
%   meta-theoretical analysis usually restricts itself to a subset of the
%   language, known as a \emph{calculus}, that contains the main features of
%   interest for the property at hand.

% \item The downside is that results for a calculus do not always carry over to
%   the full language. Problems in the calculus often reveal problems in the full
%   language, but the absence of problems in the calculus does not guarantee the
%   same for the full language.  For example, the proof of type safety
%   of~\cite{Cameron2008} for a Java calculus with generics and wildcards does not
%   carry over to Java~\cite{amin2016java}.

%     Would like to scale further from calculi to full realistic languages.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Main"
%%% End:
