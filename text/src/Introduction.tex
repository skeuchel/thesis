{ % STLCBOOL SCOPE

\newcommand{\stlcbool}{\ensuremath{\lambda_{\mathbb{B}}}\xspace}
\newcommand{\bool}{\text{bool}}
\newcommand{\true}{\textbf{true}\xspace}
\newcommand{\false}{\textbf{false}\xspace}
\newcommand{\ite}[3]{\textbf{if}~{#1}~\textbf{then}~{#2}~\textbf{else}~{#3}}
\newcommand{\typing}[3]{{#1} \vdash {#2} : {#3}}
\renewcommand{\eval}[2]{{#1} \longrightarrow {#2}}
\newcommand{\evals}[2]{{#1} \longrightarrow^* {#2}}

\chapter{Introduction}

The concept of \emph{programming} can be defined as explaining to a computer how
to perform a particular task. The language of communication is known as a
\emph{programming language} and the explanation expressed in a programming
language is called a \emph{program}; collectively programs are known as
\emph{software}. Unfortunately, programming is notoriously error-prone and
software often unreliable. Given the ubiquity of computers and the pervasive
use of software in present-day society, this unreliability is a very costly and
sometimes critical issue.

A prominent way to address this situation is to improve the programming
process. In particular, it is important that the programmer can verify whether
the program she has written implements the intended tasks. This is usually
achieved by reasoning with the \emph{expected behaviour} of a program, which of
course requires a solid understanding of the programming language in which the
program is expressed.

\paragraph{Programming Language Specifications}
In practice most programming languages do not start out as well-defined entities
with an explicit meaning. Instead they are usually indirectly defined in terms
of a software artifact like a compiler or interpreter, written in an existing
language, that processes the programs of the new language according to some
implicit intended meaning.
Only after widespread use and when the implicit definition becomes untenable, a
language may go through a standardization process where multiple stakeholders
develop a common specification of the language.
% , e.g. PHP was standardized in 2014~\cite{phpspec}.

% Another approach is develop specifications together with a reference
% implementation. These two approaches are not mutually exclusive: languages with
% an existing specification are extended in tandem with implementations.

Such a language specification has many advantages. For instance, it allows
different parties to develop software tools that process programs consistently,
and programmers to switch between tool vendors without a hitch.
Also, it allows programmers to resolve ambiguities when reasoning about her
programs independent of a particular implementation. 


\paragraph{Programming Language Theory}
The specification of programming languages is subject to scientific study in
the field of \emph{programming language theory}. This field deals with all
aspects of language specifications: the design and implementation of their
syntax, semantics and that of auxiliary systems such as type systems. In addition
the field also studies the \emph{meta-theoretical properties} (or meta-theory for short)
of languages. These meta-theoretical properties capture expectations and coherence
aspects of newly developed languages or language features, such as 
useful safety guarantees that 
hold for all programs expressed in the language. 

A prominent meta-theoretical property is \emph{type-safe}, i.e., the absence of
dynamic type errors during execution. These guarantees are usually provided for
all programs written in the language, or at least for the specific subset of
programs that have been type-checked. Because meta-theoretical properties like
type safety do not automatically hold for all programming languages, we need to
verify whether they actually hold for given languages. If they do not, this
often points to a flaw in the language's design or a misunderstanding of the
language's workings.

\paragraph{Mechanised Meta-Theory of Calculi}
Verifying meta-theoretical properties of programming languages is not a trivial
activity. Due to many subtle details and edge cases, proofs can easily become
unwieldy and getting the proof structure right requires a lot of effort. In
order to make the process more manageable, the field of programming language
theory has adopted several methodological measures.

Firstly, because full programming languages are too large to handle,
meta-theoretical analysis usually restricts itself to a subset of the language, known as
a \emph{calculus}, that contains the main features of interest for the property
at hand. Because calculi are much smaller so are the proofs of their 
meta-theory. The downside is that results for a calculus do not always
carry over to the full language. Problems in the calculus often reveal
problems in the full language, but the absence of problems in the calculus
does not guarantee the same for the full language.
For example, the Java programming language was long believed to be type-safe
\footnote{Except for some deliberate defects like the co-variance of arrays.}
but the addition of generics made Java's type system
unsound~\cite{amin2016java}. This fact was not apparent for many years, and
contradicts the type-safety results for several generic Java calculi~\cite{}.
% Another problem concerns the type-checking of Java
% programs. Generics made type-checking an undecidable property of
% programs~\cite{grigore2017}. Decidability of type-checking does not have an
% impact on properties that we get for type-checked programs, but for example it
% opens up attack vectors on (web) services that deal with Java code because the
% type checker may run forever.

Secondly, because pen-and-paper proofs are very error-prone and human reviewers
are not perfect at vetting them, meta-theoretical proofs are often written in
formal languages that can be automatically checked by software tools known as
\emph{proof assistants}. This process, known as ``mechanisation'', greatly
increases the confidence in the validity of meta-theory proofs. Unfortunately,
mechanisation does not address the large effort of proving properties, but rather
aggravates it as every little detail has to be spelled out.

\paragraph{Research Question}
Despite the mitigating efforts of the current state of the art, neither the
formal specification of programming languages nor their rigorously mechanised
meta-theoretical analysis are a widespread practice. The development costs
are still too steep to make this possible for realistic programming languages.
This leads us to the research question of this thesis:
\begin{center}
  \begin{minipage}{0.8\columnwidth}\bf
    How can we reduce the cost of mechanizing the formal meta-theory of programming
    languages?
  \end{minipage}
\end{center}

The main approach put forward in this thesis is \emph{reuse}. Reuse is a common
approach in software engineering to reduce development cost and increase both
quality and reliability. The idea is to identify functionality or patterns that
are in common between different software systems, and to implement them only once
in a manner that can be shared by the different software systems. We apply the
same idea to programming language meta-theory, identifying repeated
functionality and patterns, and implementing them only once in a way that can
be used across proofs for different languages.

% \begin{itemize}
% \item Current methodology.
% \item Why is the current methodology costly?
% \item Give an idea of the relative effort.
% \item What is the approach answer of this thesis?
% 
%   Increase reusability of code. Two strands: Modularity / Genericity.
% \end{itemize}

The remainder of this chapter provides a more detailed introduction to the
established methodology for mechanising the meta-theory of programming
languages\footnote{We refer the interested
reader to introductory textbooks on the matter (e.g., \cite{tapl}) for more
detail.} and points out opportunities for reuse. 

%% %-------------------------------------------------------------------------------
%% \section{Mechanization}
%% Mechanizing formal meta-theory in proof-assistants is crucial, both for the
%% increased confidence in complex designs and as a basis for technologies such as
%% proof-carrying code.  Formal reasoning in proof-assistants, also known as
%% mechanization, has high development costs.
%%

%% \begin{itemize}
%% \item
%%   Derive properties for all programs written in a language, like memory-safety,
%%   type safety, resources-safety, termination, absence of deadlocks, race
%%   conditions and starvation.
%%
%% \item
%%   Prove correctness (preservation of semantics) of program analyses or compiler
%%   transformations.
%%
%%   by looking formally on the semantics, type systems and implementations like
%%   compilers or interpreters.
%% \end{itemize}

%-------------------------------------------------------------------------------
\section{Programming Language Specifications}\label{sec:intro:specification}

%% Standardization and functional or program specifications are an essential
%% methodology in systems engineering and software development. It serves as the
%% baseline for correctness of implementations. Furthermore, multiple
%% implementations that are correct with respect to a common specification are
%% inter-operable or compatible to a certain extent. The precise meaning depends
%% on the context and the requirements.
%%
%% For programming languages for example, we want to guarantee that the same
%% program is executable by different implementations and performs the same
%% tasks.  This does for example also ensures that program analyses and program
%% transformations, e.g. for optimization, can be developed independent of a
%% particular implementation and adopted by multiple implementations.
%%
%% In this thesis, we are interested in programming languages themselves and not
%% so much interested in their implementations. Hence, a specification of a
%% application binary interface, for example, to achieve binary compatibility of
%% modules compiled by different compilers is beyond the scope of this thesis.

Specifications of programming languages differ in detail and precision.
Industrial specifications of major programming languages are usually written in
natural language and cover every aspect of the language in detail. Despite
the fact that natural language leaves opportunities for ambiguity, elaborate
industrial specifications provide a good reference point for language users and
implementors. However, for meta-theoretical analyses more rigorous
specifications are necessary.
For this purpose we use \emph{formal specifications} and a mathematical language to
describe programming languages. This provides the necessary precision and
avoids the ambiguities of natural languages. 

This section explains necessary
fundamental concepts for the formal specification of programming languages by
example of a small language \stlcbool: a simply-typed lambda calculus with
booleans. We specify the \emph{abstract syntax}, \emph{static type system} and
\emph{evaluation} of \stlcbool using inductive definitions. Along the way, we
define the terminology and notational conventions and make their meaning
precise.

%-------------------------------------------------------------------------------
\subsection{Syntax}\label{ssec:intro:syntax}

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
    \begin{tabular}{lcll}
      $x,y$           & ::=    &                            & term variable    \\
      $\tau,\sigma$   & ::=    &                            & type             \\
                      & $\mid$ & $\tau \to \tau$            & function type    \\
                      & $\mid$ & $\bool$                    & boolean type     \\
      $e$             & ::=    &                            & term             \\
                      & $\mid$ & $\true$                    & true constant    \\
                      & $\mid$ & $\false$                   & false constant   \\
                      & $\mid$ & $\ite{e}{e}{e}$            & conditional      \\
                      & $\mid$ & $x$                        & term variable    \\
                      & $\mid$ & $\lambda x:\tau.e$         & term abstraction \\
                      & $\mid$ & $e~e$                      & application      \\
      $v$             & ::=    &                            & term             \\
                      & $\mid$ & $\true$                    & true constant    \\
                      & $\mid$ & $\false$                   & false constant   \\
                      & $\mid$ & $\lambda x:\tau.e$         & term abstraction \\
      $\Gamma$        & ::=    &                            & type context     \\
                      & $\mid$ & $\epsilon$                 & empty context    \\
                      & $\mid$ & $\Gamma, x:\tau$           & term binding     \\
    \end{tabular}
    \end{minipage}
  }
  \caption{\stlcbool syntax}
  \label{fig:intro:stlcboolsyntax}
\end{figure}

The syntax of \stlcbool is given in Figure \ref{fig:intro:stlcboolsyntax}. We
use a convention that is close to standard (extended Backus-Naur form) grammars. Elided in the
grammar are syntactic constructs like parentheses. Yet we use parentheses freely to
resolve ambiguities in terms even if the grammar does not define them. Any
implementation that deals with the concrete syntax of a programming language has
of course to be more rigorous.

The grammar in Figure \ref{fig:intro:stlcboolsyntax} defines several
\emph{syntactic sorts} for \stlcbool. The \emph{meta-variable} $e$ stands for
expressions of \stlcbool of which there are 6 different kinds. An expression can
either be a boolean constant $\true$ or $\false$, a conditional form
$\ite{e_c}{e_t}{e_e}$, an \emph{object-language variable} (represented by the
meta-variable $x$), the definition of a function as a \textlambda-abstraction
$(\lambda x:\tau.e)$ or the application of an expression $e_1$ to another
expression $e_2$. Of course we only want to apply expressions $e_1$ that
represent functions: either by being a \textlambda-abstraction or evaluating to
one. Applying any \emph{value} other than a \textlambda-abstraction is a
\emph{type error}. We make this more precise below and come back to it in
Section \ref{sec:intro:typesafety} on analysis.

The grammar also includes the meta-variable $\tau$ that describes the types of
\stlcbool. Each \textlambda-abstraction contains a \emph{type annotation} $\tau$
for the argument variable $x$. The denotation is that the function represented
by the \textlambda-abstraction expects a value of type $\tau$ when it is
applied. We discuss types and typing contexts $\Gamma$ in more detail in Section
\ref{ssec:intro:typing} which deals with static typing.


%-------------------------------------------------------------------------------
\subsection{Semantics}\label{ssec:intro:semantics}

We have defined the syntax of \stlcbool expressions and can now turn towards
defining their meaning. There are multiple established ways to define semantics
of programming language. We can coarsely classify the approaches into three
different groups:

\begin{enumerate}
\item Operational Semantics

  Operational semantics defines the meaning of programs by specifying their
  execution in a state transition system. A \emph{state transition function} or
  a \emph{state transition relation} on the terms of the programming language
  defines the possible execution steps. The program is part of the state. For
  small languages the entire state might consist of only the program. After
  taking a step we are left with an updated state that includes a residual
  program.

  %% We can then define the meaning of programs to be the execution process on
  %% the abstract machine.


\item Denotational Semantics

  Denotational semantics defines the meaning of programs in terms of collection
  of \emph{mathematical semantic domains} that can include numbers, sets or
  functions. An \emph{interpretation function} maps program terms into these
  domains. This function is generally compositional in the syntax which
  is beneficial for modularity.

  Usually, the semantic domain has an established \emph{formal theory}. The
  theorems of the domain give rise to reasoning laws for programs. Furthermore,
  we can also derive properties of programming languages from properties of the
  collection of semantic domains.

\item Axiomatic Semantics

  Instead of deriving laws for programs from their execution behaviour or
  denotation we can also axiomatically assume these laws. This is known as an
  axiomatic semantics.

  This gives us immediately the means for reasoning about programs. Furthermore,
  we can derive a denotational semantics for the language by constructing a
  model that satisfies the chosen laws and derive properties for this model or
  even all models.

\end{enumerate}

These three approaches have different trade-offs. Denotational and axiomatic semantics
immediately give us powerful mathematical tools to reason about programming
languages and their programs, but for larger languages the required technicality
and complexity makes it extremely difficult to even define a suitable semantics.

Operational semantics do not give us the same powerful mathematical reasoning
techniques and instead impose on us the laborious task to reason about programs
by observing their behaviour during execution. However, operational semantics
are simpler and easier to define than more abstract denotational or axiomatic
semantics. Moreover, they are much closer to actual implementations. Due to the
smaller gap, operational semantics make it easier to reason about the
correctness of implementations.

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
      \framebox{\mbox{$\eval{e}{e}$}}\\
      \vspace{-5mm}
      \[ \begin{array}{c}
           \inferrule* [right=\textsc{EIfTrue},]
             {
             }
             {\eval{\ite{\true}{e_t}{e_e}}{e_t}
             } \\\\
           \inferrule* [right=\textsc{EIfFalse}]
             {
             }
             {\eval{\ite{\false}{e_t}{e_e}}{e_e}
             } \\\\
           \inferrule* [right=\textsc{EIf}]
             {\eval{e_c}{e_c'}
             }
             {\eval{\ite{e_c}{e_t}{e_e}}{\ite{e_c'}{e_t}{e_e}}
           } \\\\
           \inferrule* [right=\textsc{EAppAbs}]
             {
             }
             {\eval{(\lambda x:\tau.e_1)~e_2}{[x \mapsto e_2]e_1}
             } \\\\
           \inferrule* [right=\textsc{EApp}]
             {\eval{e_1}{e_2'}
             }
             {\eval{e_1~e_2}{e_1'~e_2}
             } \\\\
         \end{array}
       \]
    \end{minipage}
  }
  \caption{\stlcbool reduction rules}
  \label{fig:intro:stlcbooleval}
\end{figure}

For our \stlcbool language we define semantics using a \emph{small-step
  operational semantics}. This is also the approach used in Part \ref{part:gen}
of this thesis. Part \ref{part:mod} uses denotational semantics.

Figure \ref{fig:intro:stlcbooleval} gives the complete definition of the
operational semantics by means of an evaluation relation. The box in the upper
left corner \framebox{\mbox{$\eval{e_1}{e_2}$}} defines the shape and notation
that we use for the relation. In this case it is a binary relation between two
terms, which denotes that $e_1$ evaluates to $e_2$ in one step.

The remainder of the figure defines the relation using Gentzen-style inference
rules~\cite{gentzen1935}. In general, rules take the form
\[
  \inferrule* [right=\textsc{Name}]
    {J_1 \\ J_2 \\ \ldots \\ J_n
    }
    {J
    }
\]
where \textsc{Name} is an optional name for the rule that allows us to refer to
it. The meta-variable $J$ stands for judgements, which in our case are usually
mathematical statements in propositional or first-order logic. The judgements
$J_1, \ldots, J_n$ above the horizontal line are the premises of the rule, and
the judgement $J$ below the line is the conclusion. Rule without any premises
are also called \emph{axioms}. The conclusion will always involve the relation
that is being defined.

The single-step evaluation is defined using five rules. The two axioms
\textsc{EIfTrue} and \textsc{EIfFalse} show how to reduce an
\textbf{if}-expression in case the condition is either a \true or a \false
value. The case of a condition that is not yet fully evaluated is handled by
rule \textsc{EIf}. If the condition $e_c$ reduces to $e_c'$ in one step then we
can conclude the one step reduction
\[ \eval{\ite{e_c}{e_t}{e_e}}{\ite{e_c'}{e_t}{e_e}} \]

The last two rules cover the evaluation of \textlambda-terms. The rule
\textsc{EAppAbs} handles the case where the left-hand side of an application is
a \textlambda-term $(\lambda x:\tau.e_1)$. The residual program is the the body
$e_1$ of the function after substituting $e_2$ for $x$ which we write as
$[x \mapsto e_2]e_1$. The argument of the function does not have to be fully
evaluated, i.e., the rules encode a \emph{call-by-name} evaluation strategy. If
the left-hand side is not yet a \textlambda-term, we evaluate it first similarly
to \textsc{EIf}.

Note that this definition does not cover all possible cases. In particular
the case of a \textlambda-term in the condition of an \textbf{if}-expression
\[ \ite{(\lambda x:\tau.e)}{e_t}{e_e} \]
\noindent and the cases of a boolean in the left-hand side of an application
\[ \true~e_1 \qquad \text{or} \qquad \false~e_2 \]
\noindent are not specified. Since no transition is defined and the execution
is stopped without any \emph{meaningful result}, we also say that the evaluation
got stuck. In an implementation of the programming language, this corresponds to
an error that can happen during the execution of a program. It's therefore also
called a \emph{(dynamic) type error}. Programmers want to detect potential
problems like that early in the development cycle and, if possible, at compile
time. This motivates the development of \emph{static type systems}.

%-------------------------------------------------------------------------------
\subsection{Typing}\label{ssec:intro:typing}

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
      \framebox{\mbox{$\typing{\Gamma}{e}{\tau}$}}\\
      \vspace{-5mm}
      \[ \begin{array}{c}
           \inferrule* [right=\textsc{TTrue}]
             {
             }
             {\typing{\Gamma}{\true}{\bool}
             } \quad
           \inferrule* [right=\textsc{TFalse}]
             {
             }
             {\typing{\Gamma}{\false}{\bool}
             } \\\\
           \inferrule* [right=\textsc{TIf}]
             {\typing{\Gamma}{e_c}{\bool} \\
              \typing{\Gamma}{e_t}{\tau} \\
              \typing{\Gamma}{e_e}{\tau}
             }
             {\typing{\Gamma}{\ite{e_c}{e_t}{e_e}}{\tau}
             } \\\\
           \inferrule* [right=\textsc{TVar}]
             {x : \tau \in \Gamma
             }
             {\typing{\Gamma}{x}{\tau}
             } \quad
           \inferrule* [right=\textsc{TAbs}]
             {\typing{\Gamma,y:\sigma}{e}{\tau}
             }
             {\typing{\Gamma}{(\lambda y:\sigma. e)}{(\sigma\to\tau)}
             } \\\\
           \inferrule*[right=\textsc{TApp}]
             {\typing{\Gamma}{e_1}{\sigma \to \tau} \\
              \typing{\Gamma}{e_2}{\sigma}
             }
             {\typing{\Gamma}{e_1~e_2}{\tau}
             }
         \end{array}
       \]
    \end{minipage}
  }
  \caption{\stlcbool typing rules}
  \label{fig:intro:stlcbooltyping}
\end{figure}

A \emph{type system} is an assignment of types to expressions. Usually not all
expressions are typeable and un-typeable expressions are rejected. Also in some
languages there are expressions that can be assigned multiple, potentially
incomparable types. Both the partiality and the ambiguity of types suggest a
\emph{relational} rather than a \emph{functional} assignment. Such a relation is
defined in Figure \ref{fig:intro:stlcbooltyping}. It is a ternary relation
\framebox{\mbox{$\typing{\Gamma}{e}{\tau}$}} between a typing context $\Gamma$,
an expression $e$ and a type $\tau$.

The typing relation is defined using six rules. The two rules \textsc{TTrue} and
\textsc{TFalse} respectively state that the boolean constants $\true$ and
$\false$ have a boolean type. The rule \textsc{TIf} handles the case of an
\textbf{if}-expression. The three sub-expression position contain a
meta-variables $e_c$, $e_t$ and $e_e$. The premises require that the condition
$e_c$ has type boolean and the \textbf{then} and \textbf{else} branches have the
same type $\tau$. The rule then concludes that the entire \textbf{if}-expression
also has type $\tau$.

The three remaining rules deal with $\lambda$-abstractions. The typing context
$\Gamma$ is a list that associates term variables with types. In the case of a
variable, rule \textsc{TVar} looks up the corresponding type in $\Gamma$. Rule
\textsc{TAbs} checks the body of a $\lambda$-abstraction in the context
$(\Gamma,y:\sigma)$ which is the outside context $\Gamma$ extended with a pair
for the $\lambda$-bound variable $y$. The type of the $\lambda$-abstraction is
the function type $(\sigma \to \tau)$ between the argument type $\sigma$ and the
type of the body $\tau$. Finally, rule \textsc{TApp} requires that the left
expressions of an application has a function type that is compatible with the
argument.


\paragraph{Example}
Consider the boolean negation function
\[
  \lambda (y:\bool). \ite{y}{\false}{\true}
\]

This function sends booleans to booleans and should therefore have the type
$\bool\to\bool$. Giving the above typing relations, we can repeatedly apply the
rule to get a typing derivation for this. At each step only one possible rule
applies. We can arrange the rule applications in a so called \emph{derivation
  tree} that illustrates the whole derivation. Using the abbreviation
$\Gamma' := \Gamma, y:\bool$, we have the following tree:

\[
  \inferrule*
  { \inferrule*
    {
      \inferrule*{\;}{\typing{\Gamma'}{y}{\bool}} \\
      \inferrule*{\;}{\typing{\Gamma'}{\false}{\bool}} \\
      \inferrule*{\;}{\typing{\Gamma'}{\true}{\bool}} \\
    }
    {\typing{\Gamma'}{\ite{y}{\false}{\true}}{\bool}
    }
  }
  { \typing{\Gamma}{\lambda y:\bool. \ite{y}{\false}{\true}}{\bool \to \bool}
  }
\]


%-------------------------------------------------------------------------------
\section{Meta-Theoretical Analysis}\label{sec:intro:typesafety}

%% It is folklore in software development that bugs arise in both
%% implementations and in specifications. For instance, \cite{arts2015testing}
%% describes a war story on testing car software in which they found 227 bugs in
%% the \textsc{Autosar} specification (and in implementations).

As discussed before, we want to establish meta-theoretical properties that
provide us with evidence for coherence of different parts of our language and to
increase our confidence that a language is well-designed. This needs a rigorous
and formal analysis of the defined semantics of a programming language.

In this section we showcase meta-theoretical analyses with the help of our
example calculus \stlcbool. In order to prove these properties we first need to
understand how to reason about languages and their semantics. We therefore look
at standard reasoning principles for relations like our typing and evaluation
relations. We show how language properties can be expressed precisely and define
two properties for our language: determinacy and type safety. We will show how
determinacy can be proven using the reasoning principles and outline one
established way of proving type safety, namely through progress and preservation
lemmas.


\paragraph{Inductive Reasoning}
We first need to establish methods for reasoning about the evaluation and typing
of \stlcbool. Figures \ref{fig:intro:stlcbooleval} and
\ref{fig:intro:stlcbooltyping} define evaluation and typing for \stlcbool. More
precisely, our intention in both figures is to define the \emph{smallest
  relation} that includes the presented rules. This gives rise to a
\emph{structural induction principle} for the relations. Put differently, we can
induct over the shape of derivation trees (or their size or height).

As an example consider the following determinacy theorem which states that
at each point there is at most one possible successor state.
\begin{thm}[Determinacy]
  If $\eval{e_1}{e_2} \wedge \eval{e_1}{e_3}$ then $e_2 = e_3$.
\end{thm}

\begin{proof}
  The proof proceeds by induction over the derivation of $\eval{e_1}{e_2}$. For
  each possible case in the derivation of $\eval{e_1}{e_2}$ we 
  inspect the last rule that was used to derive $\eval{e_1}{e_3}$. If the same
  rule was used we can derive the equations. All other combinations lead to a
  contradiction and hence the property follows.

  We go through one of the inductive steps in detail and omit the
  others. Consider the case where $\eval{e_1}{e_2}$ was derived using
  \textsc{EIf}. So there exist expressions $e_{11}, e'_{11}, e_{12}, e_{13}$
  such that
  \begin{gather*}
    e_1 = \ite{e_{11}}{e_{12}}{e_{13}}, \\
    e_2 = \ite{e'_{11}}{e_{12}}{e_{13}} \\
    \text{and}~\eval{e_{11}}{e'_{11}}.
  \end{gather*}

  Now look at the last rule that was used to derive $\eval{e_1}{e_3}$. There are
  two possibilities: rule \textsc{EIfTrue} and rule \textsc{EIf}.

  \begin{enumerate}
  \item If rule \textsc{EIfTrue} was used, then we learn that $e_{11} = \true$
    and therefore $\eval{\true}{e'_{11}}$. However this is impossible because no
    rule evaluates $\true$ further.

  \item If rule \textsc{EIf} was used, then there exists $e''_{11}$ such that
    \begin{gather*}
      \text{and}~\eval{e_{11}}{e''_{11}}.
    \end{gather*}

    Applying the inductive hypothesis of $\eval{e_{11}}{e'_{11}}$ to the
    derivation $\eval{e_{11}}{e''_{11}}$ gives us the equality
    $e'_{11} = e''_{11}$ from which we can derive $e_2 = e_3$.
  \end{enumerate}
\end{proof}

% More precisely we have to proof
% $\forall e_3. \eval{e_1}{e_3} \Rightarrow e_2 = e_3$.


\paragraph{Type Safety}

One of the essential properties of statically-typed languages is type safety.
When working in such a language a programmer expects type guarantees when her
program is executed, e.g., all arguments passed to a function have exactly
the types indicated by the function's type signature.

We previously characterized type safety as the absence of type errors during
execution. A type error represents a state for which no further evaluation step
is defined and hence evaluation stopped.

\begin{defn}[Normal Form]
  An expression $e_1$ is a \emph{normal form} if no further execution step can be
  taken, i.e.,
  \[ \forall e_2. \neg (\eval{e_1}{e_2}) \]
\end{defn}

A type error however did not produce any \emph{meaningful result}. Let us define
what a meaningful result is. These are special expressions which are also called
values.

\begin{defn}[Value]
  Values are the subset of expressions that are defined by the following
  grammar:
  \begin{center}
    \begin{tabular}{lcll}
      $v$ & ::=    &                    & term             \\
          & $\mid$ & $\true$            & true constant    \\
          & $\mid$ & $\false$           & false constant   \\
          & $\mid$ & $\lambda x:\tau.e$ & term abstraction \\
    \end{tabular}
  \end{center}
\end{defn}

Expressions with type errors are hence normal forms that are not values. We can
make \emph{``the absence of type errors''} more precise by requiring that all
normal forms are already values.

\begin{thm}[Type Safety]
  Let $\typing{\cdot}{e_1}{\tau}$. If $e_1$ evaluates to a normal form $e_2$
  then $e_2$ is a value.
\end{thm}

Note that this definition does not require the evaluation to terminate.  A
program that runs forever without a type error is also considered type-safe.


\begin{lem}[Progress]
  Let $\typing{\cdot}{e_1}{\tau}$. Either $e_1$ is a value or we can take another
  step, i.e.
  \[ \exists e_2. \eval{e_1}{e_2}. \]
\end{lem}


\begin{lem}[Preservation]
  If $\typing{\Gamma}{e_1}{\tau}$ and $\eval{e_1}{e_2}$ then $\typing{\Gamma}{e_2}{\tau}$.
\end{lem}




\section{Mechanization}

that he expects we need to

of languages on their own are not good enough to ensure a
good design.




The undecidability of type-checking Java~\cite{grigore2017} and the unsoundness
of its type system \cite{amin2016java} might be seen as bugs of the language
specification (but no formal specification of the complete Java language
exists).


A good approach is to develop software (and programming languages) in tandem
with an implementation and refine both. However, problems usually arise in
subtle edge cases that do not appear in normal program code that we write every
day and are easy to overlook in an





and such properties have only been established for subsets of the language such
as Featherweight Java \cite{igarashi2001featherweight}.

\begin{itemize}
\item
  Improving the correctness of C compilers is a worthy goal: C code is part of
  the trusted computing base for almost every modern computer system including
  mission-critical financial servers and life- critical pacemaker firmware.
\end{itemize}
From \cite{yang2011bugs}:

\blockquote{ The striking thing about our \textsc{CompCert} results is that the
  middle-end bugs we found in all other compilers are absent. As of early 2011,
  the under-development version of \textsc{CompCert} is the only compiler we
  have tested for which \textsc{Csmith} cannot find wrong-code errors. This is
  not for lack of trying: we have devoted about six CPU-years to the task. The
  apparent unbreakability of \textsc{CompCert} supports a strong argument that
  developing compiler optimizations within a proof framework, where safety
  checks are explicit and machine-checked, has tangible benefits for compiler
  users.}


%% The meta-theory of programming language semantics and type systems is highly
%% complex due to the management of many details. Formal proofs are long and prone
%% to subtle errors that can invalidate large amounts of work. In order to
%% guarantee the correctness of formal meta-theory, techniques for mechanical
%% formalization in proof-assistants have received much attention in recent years.
%%
%% %-------------------------------------------------------------------------------
%% \section{Mechanization}
%% Mechanizing formal meta-theory in proof-assistants is crucial, both for the
%% increased confidence in complex designs and as a basis for technologies such as
%% proof-carrying code.  Formal reasoning in proof-assistants, also known as
%% mechanization, has high development costs.
%%
%% To lighten the burden of programming language mechanization, many
%% approaches have been developed that tackle the substantial boilerplate which
%% arises from variable binders. Unfortunately, the existing approaches are limited
%% in scope.
%% %% STEVEN: This is still valid, but I hope it's not misleading the reader into
%% %% thinking we deal with typing relations directly.
%% As a consequence, the human mechanizer is still unnecessarily burdened with
%% binder boilerplate and discouraged from taking on richer languages.
%%
%% This paper presents \Knot, a new approach that substantially extends the support
%% for binder boilerplate. \Knot is a highly expressive language for natural and
%% concise specification of syntax with binders. Its meta-theory constructively
%% guarantees the coverage of a considerable amount of binder boilerplate for
%% well-formed specifications, including that for well-scoping of terms and context
%% lookups. \Knot also comes with a code generator, \Needle, that specializes the
%% generic boilerplate for convenient embedding in \Coq and provides a tactic
%% library for automatically discharging proof obligations that frequently come up
%% in proofs of weakening and substitution lemmas of type systems.
%%
%% Our evaluation shows, that Needle \& Knot significantly reduce the size of
%% language mechanizations (by 40\% in our case study). Moreover, as far as we
%% know, \Knot enables the most concise mechanization of the \POPLmark Challenge
%% (1a + 2a) and is two-thirds the size of the next smallest. Finally, \Knot allows
%% us to mechanize for instance dependently-typed languages, which is notoriously
%% challenging because of dependent contexts and mutually-recursive sorts with
%% variables.
%%
%% %-------------------------------------------------------------------------------
%% \section{Binding}
%%
%% To lighten the burden of programming language mechanization, many approaches
%% have been developed that tackle the substantial boilerplate which arises from
%% variable binders. Unfortunately, existing approaches for first-order
%% representations are limited to the boilerplate that concerns the syntax of
%% languages and do not tackle common boilerplate lemmas that arise for semantic
%% relations such as typing. Consequently, the human mechanizer is still burdened
%% with proving these substantial boilerplate lemmas manually.
%%
%% %% POPL 2014 Submission
%% %%
%% %%   A key concern in the mechanization of programming language metatheory is
%% %%   the representation of terms with variable binding. The properties of
%% %%   operations manipulating terms are notoriously burdensome to prove and the
%% %%   amount of work required to scale formalizations to realistic programming
%% %%   languages with rich binding forms is deemed infeasible. This is a pity,
%% %%   because we lose the practical benefits of mechanizing real programming
%% %%   languages.
%% %%
%% %%   We present a new solution to generically handle the boilerplate involved in
%% %%   mechanizations that scales to rich binding forms and advanced rules of
%% %%   scoping. We define a new specification language for abstract syntax with
%% %%   binding and implement a code generator that produces \Coq code for the
%% %%   representation of the abstract syntax, syntactic operations and proofs of
%% %%   their properties.
%% %%
%% %%   We illustrate how our approach removes the burden of variable binding
%% %%   boilerplate in the mechanization of realistic programming languages on a
%% %%   list of example specifications and a solution of the PoplMark challenge
%% %%   based on the generated code.

} % STLCBOOL SCOPE

\section{Overview}

\begin{center}
  \begin{minipage}{0.8\columnwidth}
    Keuchel, S., \& Schrijvers, T. (2013).
    \newblock Generic Datatypes à la Carte.
    \newblock In {\em Proceedings of the 9th ACM SIGPLAN workshop on Generic
      programming}, WGP ’13, pages 13-24. ACM.
  \end{minipage}
\end{center}

\begin{center}
  \begin{minipage}{0.8\columnwidth}
    Delaware, B., Keuchel, S., Schrijvers, T., and Oliveira,
    B. C. d.~S. (2013).
    \newblock Modular Monadic Meta-Theory.
    \newblock In {\em Proceedings of the 18th ACM SIGPLAN international
      conference on Functional programming}, ICFP '13, pages 319-330. ACM.
  \end{minipage}
\end{center}

\begin{center}
  \begin{minipage}{0.8\columnwidth}
    Keuchel, S. and Schrijvers, T. (2012).
    \newblock Modular Monadic Reasoning, a (Co-)Routine.
    \newblock Presented at \emph{the 24th Symposium on Implementation and
      Application of Functional Languages}, IFL '12.
  \end{minipage}
\end{center}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Main"
%%% End:
