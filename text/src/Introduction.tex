{ % STLCBOOL SCOPE

\newcommand{\stlcbool}{\ensuremath{\lambda_{\mathbb{B}}}\xspace}
\newcommand{\bool}{\text{bool}}
\newcommand{\true}{\textbf{true}\xspace}
\newcommand{\false}{\textbf{false}\xspace}
\newcommand{\ite}[3]{\textbf{if}~{#1}~\textbf{then}~{#2}~\textbf{else}~{#3}}
\newcommand{\typing}[3]{{#1} \vdash {#2} : {#3}}
\renewcommand{\eval}[2]{{#1} \longrightarrow {#2}}
\newcommand{\evals}[2]{{#1} \longrightarrow^* {#2}}

\chapter{Introduction}

The concept of programming can be defined as communicating how to perform tasks
to a computer. An increasing number of fields such as biology, engineering,
physics, psychology and sociology rely on programming. Programming is not
limited to sciences. The ubiquity of computers and the pervasive use of software
in our society indirectly depend on programming and programmers.  However,
software is notoriously unreliable. Given the extent of our dependence on
software this is all the more detrimental and costly.

In order to improve upon this situation we need to meliorate the process of
programming itself.
\begin{itemize}
\item 'Programming languages' are the tools/methods for programming
\item The minimum requirement is that a program does what it's intended to do.
\end{itemize}

For a programmer it is essential that she can validate that a program implements
exactly the intended task which is usually achieved by reasoning with the
\emph{expected behaviour} of a program.

But how is the behaviour of programming constructs defined, or more generally,
how are programming languages defined? The majority of programming languages
start with a reference implementation that effectively defines the language.
% Examples of languages that are (still) defined this way include Perl -
% Perl~5~\cite{perllanguage} and Python~\cite{pythonlanguage}.

After widespread use, a language may go through a standardization process:
multiple stakeholders develop a common language specification.
% , e.g. PHP was standardized in 2014~\cite{phpspec}.
Another approach is develop specifications together with a reference
implementation. These two approaches are not mutually exclusive: languages with
an existing specification are extended in tandem with implementations.

Such a language specification allows a programmer to resolve ambiguities when
reasoning about her programs independent of a particular implementation. It also
enables us to switch between implementations.

Programming language theory is a branch of computer science that deals with
language specifications, design, implementation, and in particular also
meta-theoretical analysis of programming languages, their semantics and related
systems like type-systems. When developing new languages or language features we
want to make sure that the design is coherent and fulfills the expectations of
users.

The programmer is often given useful safety guarantees for this programs. For
instance, many modern programming language provide garbage collection
facilities. Yet we want our programs to be memory safe, i.e. they do not access
memory that has been freed or that has not yet been initialized. Another safety
property is type-safety, i.e. the absence of dynamic type errors during
execution. These guarantees are usually provided for all programs written in a
language or a specific subset of programs, e.g. all type-checked programs. In a
meta-theoretical analysis we want to make sure that these are indeed valid
guarantees.

The meta-theory of programming language semantics and type-systems is highly
complex due to the management of many details and edge cases. As such, it is
prone to subtle errors that can invalidate large amounts of work.

For example, the Java programming language was long believed to be type-safe
\footnote{Except for some deliberate defects like the co-variance of arrays.}
but the addition of Generics made Java's type system in fact unsound
\cite{amin2016java}. Another problem concerns the type-checking of Java
programs. Generics made type-checking an undecidable property of
programs~\cite{grigore2017}. Decidability of type-checking does not have an
impact on properties that we get for type-checked programs, but for example it
opens up attack vectors on (web) services that deal with Java code because the
type-checker may run forever.

To increase the confidence in the correctness of formal meta-theory, techniques
for formalization in proof-assistants, also known as mechanization, have
received much attention. These systems allow us to state mathematical statements
and proofs for them and let the machine check that these are in fact valid
proofs. However, the high development cost of mechanizations continues to
prevent their widespread use. This leads us to the research question that this
thesis tries to answer:

\begin{center}
  \begin{minipage}{0.8\columnwidth}
    How can we reduce the costs of mechanizing formal meta-theory of programming
    languages?
  \end{minipage}
\end{center}

\begin{itemize}
\item Current methodology.
\item Why is the current methodology costly?
\item Give an idea of the relative effort.
\item What is the approach answer of this thesis?

  Increase reusability of code. Two strands: Modularity / Genericity.
\end{itemize}




The remainder of this chapter will give a more detailed introduction to the
relevant disciplines of specification, analysis and mechanization of programming
languages similar to what can be found in introductory textbooks such as
\cite{tapl}.

%% %-------------------------------------------------------------------------------
%% \section{Mechanization}
%% Mechanizing formal meta-theory in proof-assistants is crucial, both for the
%% increased confidence in complex designs and as a basis for technologies such as
%% proof-carrying code.  Formal reasoning in proof-assistants, also known as
%% mechanization, has high development costs.
%%

%% \begin{itemize}
%% \item
%%   Derive properties for all programs written in a language, like memory-safety,
%%   type-safety, resources-safety, termination, absence of deadlocks, race
%%   conditions and starvation.
%%
%% \item
%%   Prove correctness (preservation of semantics) of program analyses or compiler
%%   transformations.
%%
%%   by looking formally on the semantics, type systems and implementations like
%%   compilers or interpreters.
%% \end{itemize}

%-------------------------------------------------------------------------------
\section{Specifications}\label{sec:intro:specification}

%% Standardization and functional or program specifications are an essential
%% methodology in systems engineering and software development. It serves as the
%% baseline for correctness of implementations. Furthermore, multiple
%% implementations that are correct with respect to a common specification are
%% inter-operable or compatible to a certain extent. The precise meaning depends
%% on the context and the requirements.
%%
%% For programming languages for example, we want to guarantee that the same
%% program is executable by different implementations and performs the same
%% tasks.  This does for example also ensures that program analyses and program
%% transformations, e.g. for optimization, can be developed independent of a
%% particular implementation and adopted by multiple implementations.
%%
%% In this thesis, we are interested in programming languages themselves and not
%% so much interested in their implementations. Hence, a specification of a
%% application binary interface, for example, to achieve binary compatibility of
%% modules compiled by different compilers is beyond the scope of this thesis.

Specifications differ in the detail and precision they describe systems.
Industrial specifications of major programming languages are usually written in
natural language and are detailed to cover every aspect of the languages. The
use of natural language leaves opportunities for ambiguity, but disregarding
this problem, elaborate industrial specifications provide a good reference point
for language users and implementors. However, for meta-theoretical analyses more
rigorous specifications are necessary.

We will use \emph{formal specifications} and a mathematical language to
describe programming languages. This gives use the necessary precision and
avoids the ambiguities of natural languages. This section explains necessary
fundamental concepts for the formal specification of programming languages by
example of a small language \stlcbool: a simply-typed lambda calculus with
booleans. We specify the \emph{abstract syntax}, \emph{static type system} and
\emph{evaluation} of \stlcbool using inductive definitions. Along the way, we
define the terminology and notational conventions and make their meaning
precise.

%-------------------------------------------------------------------------------
\subsection{Syntax}\label{ssec:intro:syntax}

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
    \begin{tabular}{lcll}
      $x,y$           & ::=    &                            & term variable    \\
      $\tau,\sigma$   & ::=    &                            & type             \\
                      & $\mid$ & $\tau \to \tau$            & function type    \\
                      & $\mid$ & $\bool$                    & boolean type     \\
      $e$             & ::=    &                            & term             \\
                      & $\mid$ & $\true$                    & true constant    \\
                      & $\mid$ & $\false$                   & false constant   \\
                      & $\mid$ & $\ite{e}{e}{e}$            & conditional      \\
                      & $\mid$ & $x$                        & term variable    \\
                      & $\mid$ & $\lambda x:\tau.e$         & term abstraction \\
                      & $\mid$ & $e~e$                      & application      \\
      $v$             & ::=    &                            & term             \\
                      & $\mid$ & $\true$                    & true constant    \\
                      & $\mid$ & $\false$                   & false constant   \\
                      & $\mid$ & $\lambda x:\tau.e$         & term abstraction \\
      $\Gamma$        & ::=    &                            & type context     \\
                      & $\mid$ & $\epsilon$                 & empty context    \\
                      & $\mid$ & $\Gamma, x:\tau$           & term binding     \\
    \end{tabular}
    \end{minipage}
  }
  \caption{\stlcbool syntax}
  \label{fig:intro:stlcboolsyntax}
\end{figure}

The syntax of \stlcbool is given in Figure \ref{fig:intro:stlcboolsyntax}. We
use a convention that is close to standard \textsc{EBnf} grammars. Elided in the
grammar are syntactic constructs like parentheses. We use parentheses freely to
resolve ambiguities in terms even if the grammar does not define them. Any
implementation that deals with the concrete syntax of a programming language has
of course to be more rigorous.

The grammar in Figure \ref{fig:intro:stlcboolsyntax} defines several
\emph{syntactic sorts} for \stlcbool. The \emph{meta-variable} $e$ stands for
expressions of \stlcbool of which there are 6 different kinds. An expression can
either be a boolean constant $\true$ or $\false$, a conditional form
$\ite{e_c}{e_t}{e_e}$, an \emph{object-language variable} (represented by the
meta-variable $x$), the definition of a function as a \textlambda-abstraction
$(\lambda x:\tau.e)$ or the application of an expression $e_1$ to another
expression $e_2$. Of course we only want to apply expressions $e_1$ that
represent functions: either by being a \textlambda-abstraction or evaluating to
one. Applying any \emph{value} other than a \textlambda-abstraction is a
\emph{type error}. We make this more precise below and come back to it it
Section \ref{sec:intro:typesafety} on analysis.

The grammar also includes the meta-variable $\tau$ that describes the types of
\stlcbool. Each \textlambda-abstraction contains a \emph{type annotation} $\tau$
for the argument variable $x$. The denotation is that the function represented
by the \textlambda-abstraction expects a value of type $\tau$ when it is
applied. We discuss types and typing contexts $\Gamma$ in more detail in Section
\ref{ssec:intro:typing} which deals with static typing.


%-------------------------------------------------------------------------------
\subsection{Semantics}\label{ssec:intro:semantics}

We have defined the syntax of \stlcbool expressions and can now turn towards
defining their meaning. There are multiple established ways to define semantics
of programming language. We can coarsely classify the approaches into three
different groups:

\begin{enumerate}
\item Operational Semantics

  Operational semantics defines the meaning of programs by specifying their
  execution in a state transition system. A \emph{state transition function} or
  a \emph{state transition relation} on the terms of the programming language
  defines the possible execution steps. The program is part of the state. For
  small languages the entire state might consist of only the program. After
  taking a step we are left with an updated state that includes a residual
  program.

  %% We can then define the meaning of programs to be the execution process on
  %% the abstract machine.


\item Denotational Semantics

  Denotational semantics defines the meaning of programs in terms of collection
  of \emph{mathematical semantic domains} that can include numbers, sets or
  functions. An \emph{interpretation function} maps program terms into these
  domains. This function is generally compositional in the syntax which
  is beneficial for modularity.

  Usually, the semantic domain has an established \emph{formal theory}. The
  theorems of the domain give rise to reasoning laws for programs. Furthermore,
  we can also derive properties of programming languages from properties of the
  collection of semantic domains.

\item Axiomatic Semantics

  Instead of deriving laws for programs from their execution behaviour or
  denotation we can also axiomatically assume these laws. This is known as an
  axiomatic semantics.

  This gives us immediately the means for reasoning about programs. Furthermore,
  we can derive a denotational semantics for the language by constructing a
  model that satisfies the chosen laws and derive properties for this model or
  even all models.

\end{enumerate}

These approaches have different trade-offs. Denotational and axiomatic semantics
immediately give us powerful mathematical tools to reason about programming
languages and their programs, but for larger languages the required technicality
and complexity makes it extremely difficult to even define a suitable semantics.

Operational semantics do not give us the same powerful mathematical reasoning
techniques and instead impose on us the laborious task to reason about programs
by observing their behaviour during execution. However, operational semantics
are simpler and easier to define than more abstract denotational or axiomatic
semantics. Moreover, they are much closer to actual implementations. Due to the
smaller gap, operational semantics make it easier to reason about the
correctness of implementations.

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
      \framebox{\mbox{$\eval{e}{e}$}}\\
      \vspace{-5mm}
      \[ \begin{array}{c}
           \inferrule* [right=\textsc{EIfTrue},]
             {
             }
             {\eval{\ite{\true}{e_t}{e_e}}{e_t}
             } \\\\
           \inferrule* [right=\textsc{EIfFalse}]
             {
             }
             {\eval{\ite{\false}{e_t}{e_e}}{e_e}
             } \\\\
           \inferrule* [right=\textsc{EIf}]
             {\eval{e_c}{e_c'}
             }
             {\eval{\ite{e_c}{e_t}{e_e}}{\ite{e_c'}{e_t}{e_e}}
           } \\\\
           \inferrule* [right=\textsc{EAppAbs}]
             {
             }
             {\eval{(\lambda x:\tau.e_1)~e_2}{[x \mapsto e_2]e_1}
             } \\\\
           \inferrule* [right=\textsc{EApp}]
             {\eval{e_1}{e_2'}
             }
             {\eval{e_1~e_2}{e_1'~e_2}
             } \\\\
         \end{array}
       \]
    \end{minipage}
  }
  \caption{\stlcbool reduction rules}
  \label{fig:intro:stlcbooleval}
\end{figure}

For our \stlcbool language we define semantics using a \emph{small-step
  operational semantics}. This is also the approach used in Part \ref{part:gen}
of this thesis. Part \ref{part:mod} uses denotational semantics.

Figure \ref{fig:intro:stlcbooleval} gives the complete definition of the
operational semantics by means of an evaluation relation. The box in the upper
left corner \framebox{\mbox{$\eval{e_1}{e_2}$}} defines the shape and notation
that we use for the relation. In this case it is a binary relation between two
terms, which denotes that $e_1$ evaluates to $e_2$ in one step.

The remainder of the figure defines the relation using Gentzen-style inference
rules~\cite{gentzen1935}. In general, rules take the form
\[
  \inferrule* [right=\textsc{Name}]
    {J_1 \\ J_2 \\ \ldots \\ J_n
    }
    {J
    }
\]
where \textsc{Name} is an optional name for the rule that allows us to refer to
it. The meta-variable $J$ stands for judgements, which in our case are usually
mathematical statements in propositional or first-order logic. The judgements
$J_1, \ldots, J_n$ above the horizontal line are the premises of the rule, and
the judgement $J$ below the line is the conclusion. Rule without any premises
are also called \emph{axioms}. The conclusion will always involve the relation
that is being defined.

The single-step evaluation is defines using five rules. The two axioms
\textsc{EIfTrue} and \textsc{EIfFalse} show how to reduce an
\textbf{if}-expression in case the condition is either a \true or a \false
value. The case of a condition that is not yet fully evaluated is handled by
rule \textsc{EIf}. If the condition $e_c$ reduces to $e_c'$ in one step then we
can conclude the one step reduction
\[ \eval{\ite{e_c}{e_t}{e_e}}{\ite{e_c'}{e_t}{e_e}} \]

The last two rules cover the evaluation of \textlambda-terms. The rule
\textsc{EAppAbs} handles the case where the left-hand side of an application is
a \textlambda-term $(\lambda x:\tau.e_1)$. The residual program is the the body
$e_1$ of the function after substituting $e_2$ for $x$ which we write as
$[x \mapsto e_2]e_1$. The argument of the function does not have to be fully
evaluated, i.e. the rules encode a \emph{call-by-name} evaluation strategy. If
the left-hand side is not yet a \textlambda-term, we evaluate it first similarly
to \textsc{EIf}.

Note that this definition does not cover all the cases. In particular
the case of a \textlambda-term in the condition of an \textbf{if}-expression
\[ \ite{(\lambda x:\tau.e)}{e_t}{e_e} \]
\noindent and the cases of a boolean in the left-hand side of an application
\[ \true~e_1 \qquad \text{or} \qquad \false~e_2 \]
\noindent are not specified. Since no transition is defined and the execution
stopped without any \emph{meaningful result}, we also say that the evaluation
got stuck. In an implementation of the programming language, this corresponds to
an error that can happen during execution of a program. It's therefore also
called a \emph{(dynamic) type error}. Programmers want to detect potential
problems like that early in the development cycle and, if possible, at compile
time. This motivates the development of \emph{static type systems}.

%-------------------------------------------------------------------------------
\subsection{Typing}\label{ssec:intro:typing}

\begin{figure}[t]
  \centering
  \fbox{
    \begin{minipage}{\columnwidth}
      \framebox{\mbox{$\typing{\Gamma}{e}{\tau}$}}\\
      \vspace{-5mm}
      \[ \begin{array}{c}
           \inferrule* [right=\textsc{TTrue}]
             {
             }
             {\typing{\Gamma}{\true}{\bool}
             } \quad
           \inferrule* [right=\textsc{TFalse}]
             {
             }
             {\typing{\Gamma}{\false}{\bool}
             } \\\\
           \inferrule* [right=\textsc{TIf}]
             {\typing{\Gamma}{e_c}{\bool} \\
              \typing{\Gamma}{e_t}{\tau} \\
              \typing{\Gamma}{e_e}{\tau}
             }
             {\typing{\Gamma}{\ite{e_c}{e_t}{e_e}}{\tau}
             } \\\\
           \inferrule* [right=\textsc{TVar}]
             {x : \tau \in \Gamma
             }
             {\typing{\Gamma}{x}{\tau}
             } \quad
           \inferrule* [right=\textsc{TAbs}]
             {\typing{\Gamma,y:\sigma}{e}{\tau}
             }
             {\typing{\Gamma}{(\lambda y:\sigma. e)}{(\sigma\to\tau)}
             } \\\\
           \inferrule*[right=\textsc{TApp}]
             {\typing{\Gamma}{e_1}{\sigma \to \tau} \\
              \typing{\Gamma}{e_2}{\sigma}
             }
             {\typing{\Gamma}{e_1~e_2}{\tau}
             }
         \end{array}
       \]
    \end{minipage}
  }
  \caption{\stlcbool typing rules}
  \label{fig:intro:stlcbooltyping}
\end{figure}

A \emph{type system} is an assignment of types to expressions. Usually not all
expressions are typeable and un-typeable expressions are rejected. Also in some
languages there are expressions that can be assigned multiple, potentially
incomparable types. Both, the partiality and the ambiguity of types, suggests a
\emph{relational} rather than a \emph{functional} assignment. Such a relation is
defined in Figure \ref{fig:intro:stlcbooltyping}. It is a ternary relation
\framebox{\mbox{$\typing{\Gamma}{e}{\tau}$}} between a typing context $\Gamma$,
an expression $e$ and a type $\tau$.

The typing relation is defined using six rules. The two rules \textsc{TTrue} and
\textsc{TFalse} respectively state that the boolean constants $\true$ and
$\false$ have a boolean type. The rule \textsc{TIf} handles the case of an
\textbf{if}-expression. The three sub-expression position contain a
meta-variables $e_c$, $e_t$ and $e_e$. The premises require that the condition
$e_c$ has type boolean and the \textbf{then} and \textbf{else} branches have the
same type $\tau$. The rule then concludes that the entire \textbf{if}-expression
also has type $\tau$.

The three remaining rules deal with $\lambda$-abstractions. The typing context
$\Gamma$ is a list that associates term variables with types. In the case of a
variable, rule \textsc{TVar} looks up the corresponding type in $\Gamma$. Rule
\textsc{TAbs} checks the body of a $\lambda$-abstraction in the context
$(\Gamma,y:\sigma)$ which is the outside context $\Gamma$ extended with a pair
for the $lambda$-bound variable $y$. The type of the $\lambda$-abstraction is
the function type $(\sigma \to \tau)$ between the argument type $\sigma$ and the
type of the body $\tau$. Finally, rule \textsc{TApp} requires that the left
expressions of an application has a function type that is compatible with the
argument.

\paragraph{Example}
Consider the boolean negation function
\[
  \lambda y:\bool. \ite{y}{\false}{\true}
\]

This function sends booleans to booleans and should therefore have the type
$\bool\to\bool$. Giving the above typing relations, we can repeatedly apply the
rule to get a typing derivation for this. At each step only one possible rule
applies. We can arrange the rule applications in a so called \emph{derivation
  tree} that illustrates the whole derivation. Using the abbreviation
$\Gamma' := \Gamma, y:\bool$, we have the following tree:

\[
  \inferrule*
  { \inferrule*
    {
      \inferrule*{\;}{\typing{\Gamma'}{y}{\bool}} \\
      \inferrule*{\;}{\typing{\Gamma'}{\false}{\bool}} \\
      \inferrule*{\;}{\typing{\Gamma'}{\true}{\bool}} \\
    }
    {\typing{\Gamma'}{\ite{y}{\false}{\true}}{\bool}
    }
  }
  { \typing{\Gamma}{\lambda y:\bool. \ite{y}{\false}{\true}}{\bool \to \bool}
  }
\]


%-------------------------------------------------------------------------------
\section{Meta-Theoretical Analysis}\label{sec:intro:typesafety}

It is folklore in software development that bugs arise in both implementations
and in specifications. For instance, \cite{arts2015testing} describes a war
story on testing car software in which they found 227 bugs in the
\textsc{Autosar} specification (and in implementations).

To ensure that our languages are well-designed we want to establish
meta-theoretical properties that ensures that all the guarantees a user expects
are indeed valid. This needs a rigorous and formal analysis of the defined
semantics of a programming language. In this section we will look at the precise
definition of the type-safety language property, present methods for reasoning
about semantics and outline how these can be used to prove the type-safety
property for for our example calculus \stlcbool.

We previously characterized type-safety as the absence of type-errors during
execution. A type-error represents a state for which no further evaluation step
is defined and hence evaluation stopped.

\begin{defn}[Normal Form]
  An expression $e_1$ is a \emph{normal form} if no further execution step can be
  taken, i.e.
  \[ \forall e_2. \neg (\eval{e_1}{e_2}) \]
\end{defn}

A type-error however did not produce any \emph{meaningful result}. Let us define
what a meaningful result is. These are special expressions which are also called
values.

\begin{defn}[Value]
  Values are the subset of expressions that are defined by the following
  grammar:
  \begin{center}
    \begin{tabular}{lcll}
      $v$ & ::=    &                    & term             \\
          & $\mid$ & $\true$            & true constant    \\
          & $\mid$ & $\false$           & false constant   \\
          & $\mid$ & $\lambda x:\tau.e$ & term abstraction \\
    \end{tabular}
  \end{center}
\end{defn}

Expressions with type-errors are hence normal forms that are not values. We can
hence make \emph{the absence of type error} more precise by requiring that all
normal forms are already values.

\begin{thm}[Type-Safety]
  Let $\typing{\cdot}{e_1}{\tau}$. If $e_1$ evaluates to a normal form $e_2$
  then $e_2$ is a value.
\end{thm}

Note that this definition does not require the evaluation to terminate.  A
program that runs forever without a type-error is also considered type-safe.


\paragraph{Inductive Reasoning}
Figures \ref{fig:intro:stlcbooleval} and \ref{fig:intro:stlcbooltyping} define
evaluation and typing for \stlcbool. More precisely, our intention is to define
the \emph{smallest relation} that includes the presented rules. This gives rise
to a \emph{structural induction principle} for the relations. Put differently,
we can induct over the shape of derivation trees (or their size or height).

%% As an example consider the following determinacy theorem which states that
%% at each point there at most one possible successor state.
%% \begin{thm}[Determinacy]
%%   If $\eval{e_1}{e_2} \wedge \eval{e_1}{e_3}$ then $e_2 = e_3$.
%% \end{thm}


\begin{lem}[Progress]
  Let $\typing{\cdot}{e_1}{\tau}$. Either $e_1$ is a value or we can take another
  step, i.e.
  \[ \exists e_2. \eval{e_1}{e_2}. \]
\end{lem}


\begin{lem}[Preservation]
  If $\typing{\Gamma}{e_1}{\tau}$ and $\eval{e_1}{e_2}$ then $\typing{\Gamma}{e_2}{\tau}$.
\end{lem}




\section{Mechanization}

that he expects we need to

of languages on their own are not good enough to ensure a
good design.




The undecidability of type-checking Java~\cite{grigore2017} and the unsoundness
of its type system \cite{amin2016java} might be seen as bugs of the language
specification (but no formal specification of the complete Java language
exists).


A good approach is to develop software (and programming languages) in tandem
with an implementation and refine both. However, problems usually arise in
subtle edge cases that do not appear in normal program code that we write every
day and are easy to overlook in an





and such properties have only been established for subsets of the language such
as Featherweight Java \cite{igarashi2001featherweight}.

\begin{itemize}
\item
  Improving the correctness of C compilers is a worthy goal: C code is part of
  the trusted computing base for almost every modern computer system including
  mission-critical financial servers and life- critical pacemaker firmware.
\end{itemize}
From \cite{yang2011bugs}:

\blockquote{ The striking thing about our \textsc{CompCert} results is that the
  middle-end bugs we found in all other compilers are absent. As of early 2011,
  the under-development version of \textsc{CompCert} is the only compiler we
  have tested for which \textsc{Csmith} cannot find wrong-code errors. This is
  not for lack of trying: we have devoted about six CPU-years to the task. The
  apparent unbreakability of \textsc{CompCert} supports a strong argument that
  developing compiler optimizations within a proof framework, where safety
  checks are explicit and machine-checked, has tangible benefits for compiler
  users.}


%% The meta-theory of programming language semantics and type-systems is highly
%% complex due to the management of many details. Formal proofs are long and prone
%% to subtle errors that can invalidate large amounts of work. In order to
%% guarantee the correctness of formal meta-theory, techniques for mechanical
%% formalization in proof-assistants have received much attention in recent years.
%%
%% %-------------------------------------------------------------------------------
%% \section{Mechanization}
%% Mechanizing formal meta-theory in proof-assistants is crucial, both for the
%% increased confidence in complex designs and as a basis for technologies such as
%% proof-carrying code.  Formal reasoning in proof-assistants, also known as
%% mechanization, has high development costs.
%%
%% To lighten the burden of programming language mechanization, many
%% approaches have been developed that tackle the substantial boilerplate which
%% arises from variable binders. Unfortunately, the existing approaches are limited
%% in scope.
%% %% STEVEN: This is still valid, but I hope it's not misleading the reader into
%% %% thinking we deal with typing relations directly.
%% As a consequence, the human mechanizer is still unnecessarily burdened with
%% binder boilerplate and discouraged from taking on richer languages.
%%
%% This paper presents \Knot, a new approach that substantially extends the support
%% for binder boilerplate. \Knot is a highly expressive language for natural and
%% concise specification of syntax with binders. Its meta-theory constructively
%% guarantees the coverage of a considerable amount of binder boilerplate for
%% well-formed specifications, including that for well-scoping of terms and context
%% lookups. \Knot also comes with a code generator, \Needle, that specializes the
%% generic boilerplate for convenient embedding in \Coq and provides a tactic
%% library for automatically discharging proof obligations that frequently come up
%% in proofs of weakening and substitution lemmas of type-systems.
%%
%% Our evaluation shows, that Needle \& Knot significantly reduce the size of
%% language mechanizations (by 40\% in our case study). Moreover, as far as we
%% know, \Knot enables the most concise mechanization of the \POPLmark Challenge
%% (1a + 2a) and is two-thirds the size of the next smallest. Finally, \Knot allows
%% us to mechanize for instance dependently-typed languages, which is notoriously
%% challenging because of dependent contexts and mutually-recursive sorts with
%% variables.
%%
%% %-------------------------------------------------------------------------------
%% \section{Binding}
%%
%% To lighten the burden of programming language mechanization, many approaches
%% have been developed that tackle the substantial boilerplate which arises from
%% variable binders. Unfortunately, existing approaches for first-order
%% representations are limited to the boilerplate that concerns the syntax of
%% languages and do not tackle common boilerplate lemmas that arise for semantic
%% relations such as typing. Consequently, the human mechanizer is still burdened
%% with proving these substantial boilerplate lemmas manually.
%%
%% %% POPL 2014 Submission
%% %%
%% %%   A key concern in the mechanization of programming language metatheory is
%% %%   the representation of terms with variable binding. The properties of
%% %%   operations manipulating terms are notoriously burdensome to prove and the
%% %%   amount of work required to scale formalizations to realistic programming
%% %%   languages with rich binding forms is deemed infeasible. This is a pity,
%% %%   because we lose the practical benefits of mechanizing real programming
%% %%   languages.
%% %%
%% %%   We present a new solution to generically handle the boilerplate involved in
%% %%   mechanizations that scales to rich binding forms and advanced rules of
%% %%   scoping. We define a new specification language for abstract syntax with
%% %%   binding and implement a code generator that produces \Coq code for the
%% %%   representation of the abstract syntax, syntactic operations and proofs of
%% %%   their properties.
%% %%
%% %%   We illustrate how our approach removes the burden of variable binding
%% %%   boilerplate in the mechanization of realistic programming languages on a
%% %%   list of example specifications and a solution of the PoplMark challenge
%% %%   based on the generated code.

} % STLCBOOL SCOPE

\section{Overview}

\begin{center}
  \begin{minipage}{0.8\columnwidth}
    Keuchel, S., \& Schrijvers, T. (2013).
    \newblock Generic Datatypes à la Carte.
    \newblock In {\em Proceedings of the 9th ACM SIGPLAN workshop on Generic
      programming}, WGP ’13, pages 13-24. ACM.
  \end{minipage}
\end{center}

\begin{center}
  \begin{minipage}{0.8\columnwidth}
    Delaware, B., Keuchel, S., Schrijvers, T., and Oliveira,
    B. C. d.~S. (2013).
    \newblock Modular Monadic Meta-Theory.
    \newblock In {\em Proceedings of the 18th ACM SIGPLAN international
      conference on Functional programming}, ICFP '13, pages 319-330. ACM.
  \end{minipage}
\end{center}

\begin{center}
  \begin{minipage}{0.8\columnwidth}
    Keuchel, S. and Schrijvers, T. (2012).
    \newblock Modular Monadic Reasoning, a (Co-)Routine.
    \newblock Presented at \emph{the 24th Symposium on Implementation and
      Application of Functional Languages}, IFL '12.
  \end{minipage}
\end{center}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Main"
%%% End:
